<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    <title>NN Experiments</title>
    <link rel="icon" href="img/favicon.ico">
    <link rel="stylesheet" href="style/styles.css">
    <link rel="stylesheet" href="style/prism.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nova+Square">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
</head>

<body>
    <header>
        <div class="wrapper">
            <div class="main_header">
                <div id="main_logo"><img src="img/uj_logo_white.png"></img></div>
                <div>
                    <h1>Inteligencja obliczeniowa</h1>
                    <p>Grupa 1.</p>
                </div>
            </div>
            <div class="menu">
                <p class="menu-item-on menu-item">Zadania</p>
                <p class="menu-item-off menu-item">Piotr Dziedzic</p>
                <p class="menu-item-off menu-item">Hubert Musiał</p>
                <p class="menu-item-off menu-item">Hubert Pamuła</p>
                <p class="menu-item-off menu-item">Tair Yerniyazov</p>
                <span class="material-symbols-outlined" id="font-changer">custom_typography</span>
            </div>
        </div>
    </header>
    <div class="content">
        <!-- GŁÓWNA STRONA -->
        <div class="wrapper page" >
            <h3>Przykład sprawozdania</h2>

            <p> Sieć Kohonena, inaczej SOM (z ang. Self Organizing Map – mapa samoorganizująca) – rodzaj sztucznej sieci neuronowej
                realizującej uczenie nienadzorowane. Zaprezentowana po raz pierwszy w 1982 roku przez fińskiego uczonego Teuvo
                Kohonena.

            <p> Jest to przykład sieci konkurencyjnej, a więc takiej, w której sygnały wyjściowe neuronów porównuje się ze sobą w
                celu
                wskazania zwycięzcy (zwycięski neuron może np. wskazywać klasyfikację sygnału wejściowego). Sieć wykorzystuje
                koncepcję
                sąsiedztwa. W wyniku uczenia tej sieci powstaje mapa topologiczna, w której neurony reprezentujące podobne klasy
                powinny
                znajdować się blisko siebie. Dzięki temu możliwe jest zaobserwowanie pewnych relacji pomiędzy klasami. Aprioryczna
                interpretacja tej mapy nie jest możliwa, gdyż sieć uczy się bez nauczyciela. Na podstawie analizy konkretnych
                przykładów
                danych wejściowych zazwyczaj można jednak ustalić, jakie znaczenie mają poszczególne rejony tej mapy.</p>
            
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                            # Importing some of the libraries needed
                            import pandas as pd                     # dataframes
                            import numpy as np                      # math operations
                            import matplotlib.pyplot as plt         # plotting charts
                            
                            # Loading the initial data set using the URL adres
                            from pathlib import Path                # for handling paths on different devices
                            from zipfile import ZipFile             # for extracting data from zip archives
                            import gdown # downloading data from Google Drive

                            # Importing Keras&TF
                            import tensorflow as tf
                            from tf import keras

                            print("The model is ready.")
                        </code></pre>
            </div>
            
            <p>Sieć uczona jest za pomocą algorytmu iteracyjnego. Na początku wagom każdego z neuronów przypisywane są losowe
                wartości.
                Następnie dla każdego obiektu ze zbioru uczącego wykonywane są następujące kroki:
            <p>Wybór zwycięskiego neuronu. Jest to ten neuron, którego wagi są najbardziej zbliżone do wektora wejściowego (wektora
                cech obiektu). Modyfikacja wag zwycięskiego neuronu w taki sposób, aby jeszcze bardziej upodobnić go do wektora
                wejściowego.
                Modyfikacja wag neuronów sąsiednich względem zwycięskiego, wyznaczonych na podstawie topologii sieci (w przypadku
                zastosowania metody „zwycięzca bierze wszystko” ten krok jest pomijany).
            
            <div class="plot"><img src="img/example.png"></img></div>
        </div>

        <!-- STRONA PIOTRA D. -->
        <div class="wrapper page">
            <h3>FFNN - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station,
                Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA M. -->
        <div class="wrapper page">
            <h3>SOM - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, 
            Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA P. -->
        <div class="wrapper page">
            <h3>FFNN - Heart Disease</h3>
            <p>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.
                In particular, the Cleveland database is the only one that has been used by ML researchers to date.
                The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0
                (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to
                distinguish presence (values 1,2,3,4) from absence (value 0).
        </div>

        <!-- STRONA TAIRA Y. -->
        <div class="wrapper page" id="main-page">
            <h2>Opis zbioru danych</h2>
            <p>Zbiór danych <b><a href="https://archive.ics.uci.edu/dataset/45/heart+disease">
            "Heart Disease (Cleveland Clinic)"</a></b> dotyczy pacjentów, część z których ma chorobę serca.
            Mamy do dyspozycji 303 instancje zawierające 14 atrybutów numerycznych i kategorialnych.
            W trakcie dokonywania krótkiej analizy (<a href='https://colab.research.google.com/drive/1dRjVmMjxnA88s9SXnwcu2DvbYx6zUVKu?usp=sharing'>notatnik Jupyter, Google Colab)</a> podczas pracy naszego zespołu nad pierwszą częścią
            projektu zauważyliśmy dosyć nierównomierny rozkład poszczególnych atrybutów. Na szczęście
            dla wielu kolumn mamy rozkłady zbliżone do normalnych, więc po standaryzacji danych cały zbiór
            będzie wyglądał sensownie z punktu widzenia trenowania sieci. 
            <div class="row-with-images">
                <div><img src="img/som_hd/dataset_distribution_1.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_2.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_3.png"></img></div>
            </div>
            Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
            uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
            przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
            posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
            (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
            odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Pobranie repozytorium
                    !pip install ucimlrepo

                    # Zapis danych w postaci pandas dataframes
                    X = heart_diseases.data.features
                    y = heart_diseases.data.targets

                    # Wyliczenie macierzy korelacji
                    corr_matrix = heart_diseases.corr()
                    corr_matrix["num"].sort_values(ascending=False)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                num 1.000000
                oldpeak 0.504092
                cp 0.407075
                exang 0.397057
                slope 0.377957
                sex 0.224469
                age 0.222853
                restecg 0.183696
                trestbps 0.157754
                chol 0.070909
                fbs 0.059186
                thalach -0.415040
                Name: num, dtype: float64
                </pre>
            </div>
            <p>Oczywiscie, niektóre atrybute są bardziej istotne, inne mniej. Teraz
                nie możemy stwierdzać nic poza określeniem współczynnika korelacji.
                Jeśli połączymy razem wartości opisujące wiek oraz puls pacjenta, uzyskamy nietrywialny wykres,
                na którym dobrze widać medianę próbek względem stosunku tych liczb:
            </p>
            <div class="plot"><img src="img/som_hd/correlation_1.png"></img></div>
            <p>Złą wiadomością jest to, że zbiór, który i tak zawiera za dużo atrybutów i za małą
                liczbę instancji, posiada rekordy cechujące się obecnością brakujących wartości
                w poszczególnych kolumnach (<code>ca</code> oraz <code>thal</code>). Mamy 2 rekordy z
                z wartościami <code>NaN</code> (Not a Number) w kolumnie <code>ca</code> oraz 4 rekordy, które
                są wypełniony tymi samymi pseudo-wartościami w kolumnie <code>thal</code>.
            </p>
            <p>Dobrą wiadomością jest stosunek liczby osób chorych do zdrowych. Mamy
                prawie że idealny rozbicie na <b>2 połówki (54% zdrowych i 46% chorych)</b>, więc
                nie musimy zastanawiać się nad metodami redukcji uprzedzeń występujących w zbiorze
                danych treningowych (idealnie oczywiście jest mieć w pełnie równomierny rozkład
                wartości przewidywanych w klasyfikacji binarnej, gdyż łatwiej będzie określić
                najniższy próg dokładności).
            </p>
            <p>Rozważmy teraz opis każdego z atrybutów danych wejściowych, które zostały dołączone
                do plików źródłowych autorów tego zbioru, gdyż w kolejnym etapie nam to się przyda:
            </p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
                <ul>
                    <li><u>age</u> - wiek;</li>
                    <li><u>trestbps</u> - spoczynkowe ciśnienie krwi;</li>
                    <li><u>chol</u> - cholesterol w surowic;y</li>
                    <li><u>thalach</u> - osiągnięte maksymalne tętno;</li>
                    <li><u>oldpeak</u>- obniżenie odcinka ST wywołane wysiłkiem fizycznym w 
                        stosunku do odpoczynku;</li>
                    <li><u>cp</u> - liczba głównych naczyń (0-3) pokolorowanych metodą 
                        fluorosopii.</li>
                </ul>

                <b>Kategorialne biarne</b>:
                <ul>
                    <li><u>sex</u> - płeć;</li>
                    <li><u>fbs</u> - cukier we krwi na czczo;</li>
                    <li><u>exang</u> - dławica piersiowa wywołana wysiłkiem fizycznym.</li>
                </ul>

                <b>Kategorialne uporządkowane</b>:
                <ul>
                    <li><u>restecq</u> - spoczynkowe wyniki elektrokardiograficzne 
                        (0 - norma, 1 - anomalia, 2 - hipertrofia);</li>
                    <li><u>slope</u> - nachylenie szczytowego odcinka ST podczas wysiłku 
                        (0 - wznoszące się, 2 - płaskie, 3 - opadający).</li>
                </ul>
    
                <b>Stricte kategorialne</b>:
                <ul>
                    <li><u>cp</u> - rodzaj bólu w klatce piersiowej (cztery typy);</li>
                    <li><u>thal</u> - thal = niedokrwistość tarczowatokrwinkowa (trzy typy).</li>
                </ul>
            </p>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="#">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Przede wszystkim powinniśmy jakoś postąpić z brakującymi wartościami. Ze zwględu na to,
                że każdy rekord zawiera 13 cech, to możemy te dwie znane nam kolumny douzupełnić wartością
                średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                303 instancje, ani nie stać nas na wyrzucenie całych tych kolumn, gdyż już na etapie
                sprawdzenie zależności liniowych (korelacji) z atrybutem przewidywanym wykazały się
                dosyć poważnie. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Imputacja wartości
                    from sklearn.impute import SimpleImputer
                    import numpy as np
                    
                    imputer = SimpleImputer(missing_values = np.NaN, strategy="median")
                    imputer.fit(X[['ca', 'thal']])
                    imputed_X_columns = imputer.transform(X[['ca', 'thal']])
                    
                    X.loc[:, ('ca', 'thal')] = imputed_X_columns
                </code></pre>
            </div>
        </div>
    </div>

    <footer>
        <div class="wrapper">
            <p>&copy; 2023 Piotr Dziedzic, Hubert Musiał, Hubert Pamuła, Tair Yerniyazov</p>
        </div>
    </footer>

    <script src="script/script.js"></script>
    <script src="script/prism.js"></script>
</body>

</html>