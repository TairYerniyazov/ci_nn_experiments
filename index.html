<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    <title>NN Experiments</title>
    <link rel="icon" href="img/favicon.ico">
    <link rel="stylesheet" href="style/styles.css">
    <link rel="stylesheet" href="style/prism.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nova+Square">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
</head>

<body>
    <header>
        <div class="wrapper">
            <div class="main_header">
                <div id="main_logo"><img src="img/uj_logo_white.png"></img></div>
                <div>
                    <h1>Inteligencja obliczeniowa</h1>
                    <p>Grupa 1.</p>
                </div>
            </div>
            <div class="menu">
                <p class="menu-item-on menu-item">Zadania</p>
                <p class="menu-item-off menu-item">Piotr Dziedzic</p>
                <p class="menu-item-off menu-item">Hubert Musiał</p>
                <p class="menu-item-off menu-item">Hubert Pamuła</p>
                <p class="menu-item-off menu-item">Tair Yerniyazov</p>
                <span class="material-symbols-outlined" id="font-changer">custom_typography</span>
            </div>
        </div>
    </header>
    <div class="content">
        <!-- GŁÓWNA STRONA -->
        <div class="wrapper page" id="main-page">
            <h2>Zadania</h2>
            <p>Dla obu wybranych zbiorów danych należy zaproponować sieć Feedforward oraz sieć SOM.
                Zadanie polega na przeprowadzeniu, opisaniu i podsumowaniu 4 eksperymentów.
            </p>
            <ol>
                <li><b>Opis zbiorów danych:</b> należy scharakteryzować zbiory danych:
                    <ul>
                        <li>Palmer Penguins;</li>
                        <li>Heart Disease;</li>
                    </ul>
                <li><b>Przygotowanie danych:</b> 
                    <ul>
                        <li>Opis wstępnej obróbki danych i sposobu podania ich sieci/kodowania - wpływ na liczbę neuronów.</li>
                        <li>Podział na zbiory: treningowy, testowy i walidujący (jak i dlaczego tak). 
                            Być może konieczne jest uwzględnienie rodzaju
                            sieci. Jeśli tak to dlaczego, jeśli nie to dlaczego?</li>
                    </ul>
                </li>
                <li><b>Struktura sieci:</b> przedstawić wybrane na początku konfiguracje obu sieci (dla obu problemów). 
                    Wszystkie decyzje uzasadnić.</li>
                <li><b>Uczenie sieci:</b> wskazać wybrane algorytmy i parametry uczenia sieci. Przedstawić osiągane rezultaty. 
                    Decyzje uzasadnić.</li>
                <li><b>Wyniki</b>: ocenić osiągnięte wyniki tj. dokładność i precyzję, złożoność i zbieżność (wykład). Tam, gdzie się da
                przedstawić i zinterpretować krzywą ROC.</li>
                <li><b>Dostrajanie parametrów:</b> krytyczna część prac, czyli co, czemu i jak było zmieniane, jakie dawało to efekty – dlaczego?</li>
                <li><b>Podsumowanie:</b> na zakończenie należy podsumować otrzymane wyniki dla 4 sieci. Zestawić je parami: raz dla danych, raz dla
                rodzaju sieci i wyciągnąć wnioski z otrzymanych wyników.</li>
            </ol>
        </div>

        <!-- STRONA PIOTRA D. -->
        <div class="wrapper page">
            <h3>FFNN - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station,
                Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA M. -->
        <div class="wrapper page">
            <h3>SOM - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, 
            Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA P. -->
        <div class="wrapper page">
            <h3>FFNN - Heart Disease</h3>
            <p>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.
                In particular, the Cleveland database is the only one that has been used by ML researchers to date.
                The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0
                (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to
                distinguish presence (values 1,2,3,4) from absence (value 0).
        </div>

        <!-- STRONA TAIRA Y. -->
        <div class="wrapper page">
            <h2>Opis zbioru danych</h2>
            <p>Zbiór danych <b><a href="https://archive.ics.uci.edu/dataset/45/heart+disease">
            "Heart Disease (Cleveland Clinic)"</a></b> dotyczy pacjentów, część z których ma chorobę serca.
            Mamy do dyspozycji 303 instancje zawierające 14 atrybutów numerycznych i kategorialnych.
            W trakcie dokonywania krótkiej analizy (<a href='https://colab.research.google.com/drive/1dRjVmMjxnA88s9SXnwcu2DvbYx6zUVKu?usp=sharing'>notatnik Jupyter, Google Colab)</a> podczas pracy naszego zespołu nad pierwszą częścią
            projektu zauważyliśmy dosyć nierównomierny rozkład poszczególnych atrybutów. Na szczęście
            dla wielu kolumn mamy rozkłady zbliżone do normalnych, więc po standaryzacji danych cały zbiór
            będzie wyglądał sensownie z punktu widzenia trenowania sieci. 
            <div class="row-with-images">
                <div><img src="img/som_hd/dataset_distribution_1.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_2.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_3.png"></img></div>
            </div>
            Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
            uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
            przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
            posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
            (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
            odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Pobranie repozytorium
                    !pip install ucimlrepo

                    # Zapis danych w postaci pandas dataframes
                    X = heart_diseases.data.features
                    y = heart_diseases.data.targets

                    # Wyliczenie macierzy korelacji
                    corr_matrix = heart_diseases.corr()
                    corr_matrix["num"].sort_values(ascending=False)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                num 1.000000
                oldpeak 0.504092
                cp 0.407075
                exang 0.397057
                slope 0.377957
                sex 0.224469
                age 0.222853
                restecg 0.183696
                trestbps 0.157754
                chol 0.070909
                fbs 0.059186
                thalach -0.415040
                Name: num, dtype: float64
                </pre>
            </div>
            <p>Złą wiadomością jest to, że zbiór, który i tak zawiera za dużo atrybutów i za małą
                liczbę instancji, posiada rekordy cechujące się obecnością brakujących wartości
                w poszczególnych kolumnach (<code>ca</code> oraz <code>thal</code>). Mamy 2 rekordy z
                z wartościami <code>NaN</code> (Not a Number) w kolumnie <code>ca</code> oraz 4 rekordy, które
                są wypełniony tymi samymi pseudo-wartościami w kolumnie <code>thal</code>.
            </p>
            <p>Dobrą wiadomością jest stosunek liczby osób chorych do zdrowych. Mamy
                prawie że idealny rozbicie na <b>2 połówki (54% zdrowych i 46% chorych)</b>, więc
                nie musimy zastanawiać się nad metodami redukcji uprzedzeń występujących w zbiorze
                danych treningowych (idealnie oczywiście jest mieć w pełnie równomierny rozkład
                wartości przewidywanych w klasyfikacji binarnej, gdyż łatwiej będzie określić
                najniższy próg dokładności).
            </p>
            <p>Rozważmy teraz opis każdego z atrybutów danych wejściowych, które zostały dołączone
                do plików źródłowych autorów tego zbioru, gdyż w kolejnym etapie nam to się przyda:
            </p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
                <ul>
                    <li><u>age</u> - wiek;</li>
                    <li><u>trestbps</u> - spoczynkowe ciśnienie krwi;</li>
                    <li><u>chol</u> - cholesterol w surowicy</li>
                    <li><u>thalach</u> - osiągnięte maksymalne tętno;</li>
                    <li><u>oldpeak</u>- obniżenie odcinka ST wywołane wysiłkiem fizycznym w 
                        stosunku do odpoczynku;</li>
                    <li><u>cp</u> - liczba głównych naczyń (0-3) pokolorowanych metodą 
                        fluorosopii.</li>
                </ul>

                <b>Kategorialne biarne</b>:
                <ul>
                    <li><u>sex</u> - płeć;</li>
                    <li><u>fbs</u> - cukier we krwi na czczo;</li>
                    <li><u>exang</u> - dławica piersiowa wywołana wysiłkiem fizycznym.</li>
                </ul>

                <b>Kategorialne uporządkowane</b>:
                <ul>
                    <li><u>restecq</u> - spoczynkowe wyniki elektrokardiograficzne 
                        (0 - norma, 1 - anomalia, 2 - hipertrofia);</li>
                    <li><u>slope</u> - nachylenie szczytowego odcinka ST podczas wysiłku 
                        (0 - wznoszące się, 2 - płaskie, 3 - opadający).</li>
                </ul>
    
                <b>Stricte kategorialne</b>:
                <ul>
                    <li><u>cp</u> - rodzaj bólu w klatce piersiowej (cztery typy);</li>
                    <li><u>thal</u> - thal = niedokrwistość tarczowatokrwinkowa (trzy typy).</li>
                </ul>
            </p>
            <p>Oczywiscie, niektóre atrybute są bardziej istotne, inne mniej. Teraz
                nie możemy stwierdzać nic poza określeniem współczynnika korelacji.
                Jeśli połączymy razem wartości opisujące wiek oraz puls pacjenta, uzyskamy nietrywialny wykres,
                na którym dobrze widać medianę próbek względem stosunku tych liczb:
            </p>
            <div class="plot"><img src="img/som_hd/correlation_1.png"></img></div>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="#">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Przede wszystkim powinniśmy jakoś postąpić z brakującymi wartościami. Ze zwględu na to,
                że każdy rekord zawiera 13 cech, to możemy te dwie znane nam kolumny douzupełnić wartością
                średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                303 instancje, ani nie stać nas na wyrzucenie całych tych kolumn, gdyż już na etapie
                sprawdzenie zależności liniowych (korelacji) z atrybutem przewidywanym wykazały się
                dosyć poważnie. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Rzutowanie wszystkich kolumn na liczby zmiennoprzecinkowe
                    X = X.astype("float")

                    # Imputacja wartości
                    from sklearn.impute import SimpleImputer
                    import numpy as np
                    
                    imputer = SimpleImputer(missing_values = np.NaN, strategy="median")
                    imputer.fit(X[['ca', 'thal']])
                    imputed_X_columns = imputer.transform(X[['ca', 'thal']])
                    
                    X.loc[:, ('ca', 'thal')] = imputed_X_columns
                </code></pre>
            </div>
            <p>Kolejna rzecz, którą musimy załatwić, polega na przerzucenia wartości przewidywanego
                atrybutu na liczby 0 i 1, gdyż wszystko powyżej 0 wskazuje na obecność choroby i nie 
                dodaje żadnego innego znaczenia. Zróbmy to teraz:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Zamiana przewidywanego atrybutu na wartości binarne zgodnie z opisem zbioru
                    y.loc[y['num'] > 0, 'num'] = 1.0
                </code></pre>
            </div>
            </p>Następnie na naszej liście zadań do zrobienia mamy oczwywiście przetworzenie atrybutów
            kategorialnych. Tak jak wspomnieliśmy wcześniej, niektóre z nich są binarne, więc z nimi
            nie będzie żadnego problemu, gdyż mieszczą się w przedziale od 0 do 1. Inne mają porządek
            semantyczny i określają stopień konkretnej cechy, więc też je możemy bezpiecznie zostawić, a
            później standaryzować jako atrybuty numeryczne. Zostaje nam problem tych atrybutów, które
            są stricte kategorialne, tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
            konkretnych cech.
            <p>Możemy te atrybutu (<code>cp</code> oraz <code>thal</code>) rozbić na kilka kolumn.
            Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
            Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
            (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który to robi
            dla atrybutu <code>cp</code>:
            </p> 
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X_cp = X[['cp']]
                    
                    # Zakodujmy najpierw "gorącojedynkowo" wszystkie cztery typy 'cp'
                    from sklearn.preprocessing import OneHotEncoder
                    
                    cat_encoder = OneHotEncoder()
                    X_cp_1hot = cat_encoder.fit_transform(X_cp)

                    # Teraz mamy dodatkowe 4 kolumny (każda odpowiada za obecność 
                    # określonego rodzaju bólu w klatce piersiowej):
                    print(X_cp_1hot.toarray())
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    [[1. 0. 0. 0.]
                    [0. 0. 0. 1.]
                    [0. 0. 0. 1.]
                    ...
                    [0. 0. 0. 1.]
                    [0. 1. 0. 0.]
                    [0. 0. 1. 0.]]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Pierwotne etykiety (numery typu bólu, 1-4) mamy wciąż pod ręką
                    cat_encoder.categories_
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    [array([1., 2., 3., 4.])]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Dodajmy teraz te typy zakodowane binarnie do naszych danych i wyrzućmy kolumnę 'cp'
                    for i in range(4):
                        X[f"cp_type_{i + 1}"] = X_cp_1hot.toarray()[:, i].reshape(-1, 1)
                    X.drop(columns=["cp"], inplace=True)
                </code></pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w 
                tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych (oprócz atrybutów binarnych):
            <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                from sklearn.model_selection import train_test_split
        
                # Podział na zbióry: testowy, treningowy
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)
                
                # Standaryzacja
                from sklearn.preprocessing import StandardScaler

                num_columns = ['age', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca']
                cat_columns = ['sex', 'fbs', 'exang', 'cp_type_1', 'cp_type_2', 'cp_type_3', 'cp_type_4', 'thal_3', 'thal_6', 'thal_7']
                X_train_cat = X_train.loc[:, cat_columns]
                X_train_num = X_train.loc[:, num_columns]

                std_scaler = StandardScaler()
                X_train_num_scaled = std_scaler.fit_transform(X_train_num)
                X_train = np.concatenate((X_train_num_scaled, X_train_cat.to_numpy()), axis=1)
                
                y_train = y_train.to_numpy()
            </code></pre></div>
            <p>Jeśli chodzi o dane do walidacji, to sensownym rozwiązaniem wydaje się być
                stosowanie K-składową walidacji krzyżowej (z ang. <b>K-Fold Cross Validation</b>):
                <div class="plot"><img src="img/som_hd/k_fold_validation.png"></img></div>
                <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                    k_folds = 5
                    num_val_samples = len(X_train) // k_folds
                    num_last_val_samples = len(X_train) % k_folds
                    
                    X_validation_sets = []
                    y_validation_sets = []
                    
                    X_train_sets = []
                    y_train_sets = []
                    
                    for i in range(k_folds):
                        print('Processing fold #', i)
                        X_validation_sets.append(X_train[i * num_val_samples: (i + 1) * num_val_samples])
                        y_validation_sets.append(y_train[i * num_val_samples: (i + 1) * num_val_samples])
                        X_train_sets.append(np.concatenate([X_train[:i * num_val_samples],
                        X_train[(i + 1) * num_val_samples:]], 
                        axis=0))
                        y_train_sets.append(np.concatenate([y_train[:i * num_val_samples],
                        y_train[(i + 1) * num_val_samples:]],
                        axis=0))
                    
                        # Dodajemy resztę do ostaniego zbioru walidacyjnego
                        if i == (k_folds - 1):
                        X_validation_sets[k_folds - 1] = np.concatenate((X_validation_sets[k_folds - 1], X_train[- num_last_val_samples:]),
                        axis=0)
                        y_validation_sets[k_folds - 1] = np.concatenate((y_validation_sets[k_folds - 1], y_train[- num_last_val_samples:]),
                        axis=0)
                </code></pre></div>
            <p>Teraz bodajże wszystkie atrybuty mają wartości przedstawione numerycznie, 
                a te, które określają kategorie, mają albo porządek, albo są zakodowane binarnie. 
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci SOM</h2>
            <p>Wiemy, oczywiście, że mamy do czynienia z problemem klasyfikacji, więc moglibyśmy 
                od razu w przestrzeni neuronów ustawić tylko dwie jednostki (jedna odpowiadałaby
                wówczas za osób chorych, inny za zdrowych). Jednak nie będzie to
                prawdziwa SOM, tylko zwyczajne wykonanie algorytmu LVQ, stosując uczenie nadzorowane.
        
            <p>Spróbujmy zatem postąpić inaczej. Wyobraźmy sobie, że nie wiemy, że mamy dane naszych 
                pacjęntów i nie wiemy, czy wgl oni się wyodrębniają w jakieś osobne kategorie czy nie. 
                Wtedy mamy uczenie nienadzorowane, więc klastry "osoby chore" i "osoby zdrowe" 
                powstaną naturalnie (co najmniej w teorii, bo możliwe, że sięc nam zamiast tego np.
                rozdzieli ludzi na kobiet i mężczyzn, ale to jest mniej prawdopodobne, bo 17 kolumn 
                wskazują prawdopodobnie na to, że płeć nie stanowi za dużą różnicę w przypadku danych
                medycznych dotyczących chorób serca).

            <p><b>Architektura sieci:<b><p>
            <ul>
                <li><u>Siatka</u> - 2D, 9x9;</li>
                <li><u>Liczba neuronów</u> - 81 (zasada heurystyczna dla treningowego zbioru danych
                    o rozmiarze 272 rekordy);</li>
                <li><u>Wymiar wektorów wag</u> - 18 (co odpowiada liczbie atrybutów);</li>
                <li><u>Odległość</u> - metryka Euklidesowa</li>
                <li><u>Liczenie sąsiedztwa</u> - metryka Manhattan;</li>
                <li><u>Redukcja współczynników</u> - zależy od maksymalnych wartości oraz całkowitej
                liczby iteracji.</li>
            </ul>
            <div class="plot"><img src="img/som_hd/som_nn.png"></img></div>
        </div>
    </div>

    <footer>
        <div class="wrapper">
            <p>&copy; 2023 Piotr Dziedzic, Hubert Musiał, Hubert Pamuła, Tair Yerniyazov</p>
        </div>
    </footer>

    <script src="script/script.js"></script>
    <script src="script/prism.js"></script>
</body>

</html>