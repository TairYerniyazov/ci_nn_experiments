<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    <title>NN Experiments</title>
    <link rel="icon" href="img/favicon.ico">
    <link rel="stylesheet" href="style/styles.css">
    <link rel="stylesheet" href="style/prism.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nova+Square">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
</head>

<body>
    <header>
        <div class="wrapper">
            <div class="main_header">
                <div id="main_logo"><img src="img/uj_logo_white.png"></img></div>
                <div>
                    <h1>Inteligencja obliczeniowa</h1>
                    <p>Grupa 1.</p>
                </div>
            </div>
            <div class="menu">
                <p class="menu-item-on menu-item">Zadania</p>
                <p class="menu-item-off menu-item">Piotr Dziedzic</p>
                <p class="menu-item-off menu-item">Hubert Musiał</p>
                <p class="menu-item-off menu-item">Hubert Pamuła</p>
                <p class="menu-item-off menu-item">Tair Yerniyazov</p>
                <span class="material-symbols-outlined" id="font-changer">custom_typography</span>
            </div>
        </div>
    </header>
    <div class="content">
        <!-- GŁÓWNA STRONA -->
        <div class="wrapper page" id="main-page">
            <h2>Zadania</h2>
            <p>Dla obu wybranych zbiorów danych należy zaproponować sieć Feedforward oraz sieć SOM.
                Zadanie polega na przeprowadzeniu, opisaniu i podsumowaniu 4 eksperymentów.
            </p>
            <ol>
                <li><b>Opis zbiorów danych:</b> należy scharakteryzować zbiory danych:
                    <ul>
                        <li>Palmer Penguins;</li>
                        <li>Heart Disease;</li>
                    </ul>
                <li><b>Przygotowanie danych:</b> 
                    <ul>
                        <li>Opis wstępnej obróbki danych i sposobu podania ich sieci/kodowania - wpływ na liczbę neuronów.</li>
                        <li>Podział na zbiory: treningowy, testowy i walidujący (jak i dlaczego tak). 
                            Być może konieczne jest uwzględnienie rodzaju
                            sieci. Jeśli tak to dlaczego, jeśli nie to dlaczego?</li>
                    </ul>
                </li>
                <li><b>Struktura sieci:</b> przedstawić wybrane na początku konfiguracje obu sieci (dla obu problemów). 
                    Wszystkie decyzje uzasadnić.</li>
                <li><b>Uczenie sieci:</b> wskazać wybrane algorytmy i parametry uczenia sieci. Przedstawić osiągane rezultaty. 
                    Decyzje uzasadnić.</li>
                <li><b>Wyniki</b>: ocenić osiągnięte wyniki tj. dokładność i precyzję, złożoność i zbieżność (wykład). Tam, gdzie się da
                przedstawić i zinterpretować krzywą ROC.</li>
                <li><b>Dostrajanie parametrów:</b> krytyczna część prac, czyli co, czemu i jak było zmieniane, jakie dawało to efekty – dlaczego?</li>
                <li><b>Podsumowanie:</b> na zakończenie należy podsumować otrzymane wyniki dla 4 sieci. Zestawić je parami: raz dla danych, raz dla
                rodzaju sieci i wyciągnąć wnioski z otrzymanych wyników.</li>
            </ol>
            <h2>Notatniki Jupyter</h2>
            <p>Google Colab</p>
            <ol>
                <li><a href="https://colab.research.google.com/drive/1baNi0ls9fdn9ztc2AR0PuS4e6hMK1em0?usp=sharing">FFNN Palmer Penguins</a></li>
                <li><a href="https://colab.research.google.com/drive/1E5rqWA-NZe-8QkHHYTKL1LNFD2nP7Il9?usp=sharing">SOM Palmer Penguins</a></li>
                <li><a href="#">FFNN Heart Disease</a></li>
                <li><a href="https://colab.research.google.com/drive/1aFp_SQcUAk8FIX2B24jQNYbMutnl50P_?usp=sharing">SOM Heart Disease</a></li>
            </ol>
            <h2>Wyniki</h2>
            <p>Próbując rozwiązać problem klasyfikacji binarnej oraz wieloklasowej udało nam się zbudować różne sieci neuronowe - Self-organising map (SOM)
                oraz (Shallow) Feedforward Neural Network (FFNN). Spróbujmy teraz porównać ich skuteczność oraz wyjaśnić, na czym polegała różnica:<p>
                    <un> 
                        <li>Obydwa zbiory danych zostały przedstawione i zbadane w trakcie eksperymentów. Wstępne przetworzenie danych
                            oraz inżynieria cech pozwoliły na uzyskanie ciekawych wyników.</li><br>
                        <li><b>Sieć SOM & zbiór Health Disease:</b> zaprojektowaliśmy i zaimplementowaliśmy całą architekturę od zera, nie korzystając z
                            żadnych bibliotek. Udało się wyodrębnić dane i utworzyć dwa klastry. Dzięki temu, że zbadaliśmy wygląd macierzy
                            odległości, mogliśmy zaobserwować tworzenie "ścian" separujących próbki i wykrywających pewne zależności.
                            Na szczęście właściwe klastry, które zostały wykryty, zostały później skutecznie rozpoznane jako osoby zdrowe i chore.
                            Później zastosowaliśmy wyuczone neurony do klasyfikacji binarnej i uzyskaliśmy po 6k iteracji dokładność na poziomie <b>81%</b>.
                        </li><br>
                        <li><b>Sieć SOM & zbiór Palmer Penguin:</b> zaprojektowaliśmy sieć, a następnie zaimplementowaliśmy, korzystajć z biblioteki minisom.
                            Wizualizacja macierzy odległości złączonej z macierzą odzwierciedlającą zaznaczone przez nas próbki pozwoliły na zaobserwowanie
                            trzech powstałych grup. Każda z tych grup reprezentowała konkretny gatunek pingwina. Właśnie tego się spodziewaliśmy.
                            Stosując 5-składową walidację krzyżową oraz siatkę o rozmiarze 10x10 uzyskaliśmy dokładność na poziomie <b>73%</b>. 
                        </li><br>
                        <li><b>Sieć FFNN & zbiór Health Disease:</b> sieć dobrze sprawdziła się w przypadku klasyfikacji biarnej. Próbowaliśmy
                        na różne sposoby uzyskać jak najszybszą zbieżność i równocześnie uniknąć nadmiernego dopasowania. Stosowanie takich
                        metod jak wprowadzenie warstwy odrzucania (Dropout Layer) pomagało utrzymywać wysoką dokładność i precyzję klasyfikacji zarówno
                        na zbiorze testowym, jak i walidacyjnym. Dzięki temu ostateczny model osiągnął dokładność na poziomie <b>90%</b>, mimo że wyuczony
                        został na bardzo małej liczbie danych i małej liczbie parametrów.</li><br>
                        <li><b>Sieć FFNN & zbiór Palmer Penguin:</b> sieć, która posiada zaledwie 10 neuronów w jednej warstwie ukrytej, potrafiła
                            osiągnąć <b>100%</b> dokładność, próbując rozwiązać problem klasyfikacji wieloklasowej. Taka sieć nie jest złożona ani nie 
                            posiada za dużo parametrów, więc jak najbardziej nadała się do małego zbioru danych, wykazując się zdolnością do
                            generalizacji. Wszystkie hiperparametry w tym eksperymencie zostały uważnie dobrane. Stosowanie optymalizatora Adam
                            opierającego się o liczenie pędu wektorów gradientu pozwoliło na szybkie znalezienie minimum funkcji straty określoną
                            przez entropię krzyżową. Krzywe ROC są idealne, co może wynikać z tego, że podczas ewaluacji sieci na zbiorze testowym
                            model miał pewność, że rozpoznaje właściwy gatunek pingwina. Innymi słowy, mieliśmy dobrą precyzję.</li><br>
                        <li><b>Sieć FFNN vs sieć SOM:</b> w trakcie eksperymentów zobaczyliśmy, że nie zawsze jest sens stosować mapy samoorganizujące się
                            do klasyfikacji bądź innych metod opartych na <b>uczeniu nadzorowanym</b>. Radzą sobie dobrze w przypadku surowej eksploracji
                            danych - czyli w uczeniu <b>nienadzorowanym</b> (np. widzieliśmy, jak ze zbioru Heart Disease powstają klastry, 
                            a nie wiemy, czy na pewno chodziło o obecność choroby). 
                            Sieci FFNN są bardziej uniwersalne, ale z kolei w ramach naszych eksperymentów nie wychodziły poza
                            obszar uczenia nadzorowanego. Sieci gęsto połączone poradziły sobie dobrze (mając przy tym bardzo małe wymagania
                            obliczeniowe oraz ograniczony zestaw parametrów) w obydwu przypadkach. Oczywiście głównym zadaniem dla nas było też
                            kontrolowanie całego procesu uczenia się, gdyż sieć mogła w dowolnej chwili zapamiętać na sztywno dane wejściowe,
                            przez co otrzymalibyśmy brak zdolności do uogólnienia, wykrywania wzorców, ekstrakcji przydatnych cech. 
                        </li>
                    </un><br>    
        </div>

        <!-- STRONA PIOTRA D. -->
        <div class="wrapper page">
            <div class="plot plot-tiny-width"><img src="img/penguins_logo.png"></img></div>
            <h2>Opis zbioru danych</h2>
            <p>Zbiór został stworzony i udostępniony przez dr Kristen Gorman i stację Palmer Station na Antarktydzie,
                członka Długoterminowej Sieci Badań Ekologicznych wspieranego przez granty za pośrednictwem
                National Science Foundation, Office of Polar Programs (NSF-OOP). Dane zebrano w ramach badań mających
                na celu zbadanie zachowań żerowych pingwinów antarktycznych i ich związku ze zmiennością środowiska</p>
            <p>Dane pierwotnie zamieszczono w 3 oddzielnych zbiorach. Każdy odpowiadał jednemu z trzech gatunków
                Adelie (152 pingwiny), Gentoo (124 pingwiny) i Chinstrap 68 pingwinów. Połączono je w jeden zbiór danych
                o pingiwnach Palmera (344 pingwiny)
            </p>
            <div class="plot plot-small-height"><img src="img/ff_penguins/penguins_count.png"></img></div>
            <p>Mamy zatem 344 instacncje zawierające 8 atrybutów numerycznych i kategorycznych. Przedstawmy teraz krótko
                każdy z nich, w celu lepszego zrozumienia zagadnienia</p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
            <ul>
                <li><u>bill_length_mm</u> - Długość dzioba</li>
                <li><u>bill_depth_mm</u> - Wysokość dzioba</li>
                <li><u>flipper_length_mm</u> - Długość płetwy</li>
                <li><u>body_mass_g</u> - Masa Ciała pingwina</li>
            </ul>

            <b>Kategoryczne biarne</b>:
            <ul>
                <li><u>sex</u> - Płeć pingwina</li>
            </ul>

            <b>Kategoryczne</b>:
            <ul>
                <li><u>year</u> - Rok w którym pingwin został zbadany</li>
                <li><u>Species</u> - Gatunek pingwina</li>
                <li><u>Island</u> - Wyspa którą pingwin zamieszkiwał</li>
            </ul>
            </p>
            <p>Niestety 2 atrubuty "Year" oraz "Island" są dla nas bezużytczne w problemie klasyfikacji więc nie bierzemy
                ich pod uwagę w dalszych rozważaniach. Zatem liczba naszych "features" tak naprawdę zredukowała się do 5</p>
            <div class="plot plot-tiny-width"><img src="img/ff_penguins/bill_data_example.png"></img></div>
            <p>Zbadaliśmy rozkład 3 najistotniejszych cech czyli Bill Length, Bill Depth,
                Flipper Length i otrzymaliśmy następujące wyniki</p>
            <div class="plot"><img src="img/ff_penguins/bill_depth_distribution.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/bill_length_distribution.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/filpper_length_distribution.png"></img></div>
            <p>Mamy dobrą dystrybucję trzech istotnych atrybutów numerycznych, która wygląda na rozkład normalny.
                Zatem, pod warunkiem, że dane ze zbioru testowego, a później produkcyjnego (przemilczmy, że klasyfikacja
                pingwinów nam się nie przyda w Krakowie....)
                będą się cechowały taką samą wariancją, to model dobrze sobie poradzi, gdyż spodziewamy się, że odchylenie
                standardowe będzie ograniczało liczbę
                tzw. "Outliers"'ów, czyli wartości odbiegających od mediany i średniej.</p>
            <p>Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
                uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
                przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
                posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
                (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
                odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.</p>
            <div class="plot"><img src="img/ff_penguins/heatmapa.png"></img></div>
            <p>W pierwszym rzędzie macierzy widać wzdłuż kolumn, że wartości liczbowe odgrywają kluczową rolę w predykcji
                atrybuty "species".
                Odwrotna obserwacja ma miejsce w przypadku płci (można przypuszczać, że female penguins nie zbyt mocno się
                różnią od male penguins).
                Dla przypomnienia: macierz korelacji (o ile współczynnik korelacji nie jest równy 0 dla któregoś atrybutu,
                który wtedy po prostu jest wartością stałą) jest zawsze symetryczna, gdyż odzwierciedla relacje liniowe, a więc
                można zawsze to odwrócić.
                Wdzłuż przekątnej są same jedynki, gdyż każdy atrybut ma 100% korelacji z samym sobą z definicji.</p>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="https://colab.research.google.com/drive/1baNi0ls9fdn9ztc2AR0PuS4e6hMK1em0?usp=sharing">notatnik Jupyter
                    (Google Colab)</a>
            </p>
            <p>Zaczniemy od przetworzenie atrybutów
                kategorycznych. Mamy tutaj atrybuty, które
                są stricte kategoryczne (sex, species), tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
                konkretnych cech.</p>
            <p>Możemy te atrybuty (<code>sex</code> oraz <code>species</code>) rozbić na kilka kolumn.
                Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
                Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
                (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który za to odpowiada
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                            # Zmiana kategorycznych wartosci na numeryczne
                            
                            # Kodowanie danych kategorycznych (one-hot encoding)
                            label_encoder = LabelEncoder()
                            
                            # One-hot encoding dla kolumny 'species'
                            palmer_penguins = pd.get_dummies(palmer_penguins, columns=['species'], prefix=['species'])
                            
                            # One-hot encoding dla kolumny 'sex'
                            palmer_penguins = pd.get_dummies(palmer_penguins, columns=['sex'], prefix='sex')
                            
                            print(palmer_penguins)
                        </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                bill_length_mm bill_depth_mm    flipper_length_mm   body_mass_g \
            168         42.0            13.5             210.0       4150.0
            37          42.2            18.5             180.0       3550.0
            269         48.8            16.2             222.0       6000.0
            156         47.6            14.5             215.0       5400.0
            226         46.4            15.0             216.0       4700.0
            ..          ...             ...              ...         ...
            89          38.9            18.8             190.0       3600.0
            277         50.0            19.5             196.0       3900.0
            182         47.3            15.3             222.0       5250.0
            338         45.7            17.0             195.0       3650.0
            221         50.7            15.0             223.0       5550.0
            
                sex_female  sex_male
            168      1.0     0.0
            37       1.0     0.0
            269      0.0     1.0
            156      0.0     1.0
            226      1.0     0.0
            ..       ...     ...
            89       1.0     0.0
            277      0.0     1.0
            182      0.0     1.0
            338      1.0     0.0
            221      0.0     1.0

            [247 rows x 6 columns]
                        </pre>
            </div>
            <p>Teraz powinniśmy się zastanowić jak postąpić z brakującymi wartościami. Możemy nieznane nam atrybuty
                douzupełnić wartością średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                344 instancje. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers">
                            <code class="language-python">
                                # Sprawdzenie, ile jest brakujących danych w każdej kolumnie
                                brakujace_dane = palmer_penguins.isnull().sum()
                    
                                # Wyświetlenie liczby brakujących danych w każdej kolumnie
                                print(brakujace_dane)
                            </code>
                        </pre>
                <code>
                            <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
            bill_length_mm          2
            bill_depth_mm           2
            flipper_length_mm       2
            body_mass_g             2
            species_Adelie          0
            species_Chinstrap       0
            species_Gentoo          0
            sex_female              0
            sex_male                0
            dtype: int64
                            </pre>
                        </code>
                <pre class="line-numbers">
                            <code class="language-python">
                                # Uzupełnianie wartości nieokreślonych średnią
                    
                                # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                                srednia_bill_length_mm = palmer_penguins['bill_length_mm'].mean()
                    
                                # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                                palmer_penguins['bill_length_mm'].fillna(srednia_bill_length_mm, inplace=True)
                    
                                # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                                srednia_bill_depth_mm = palmer_penguins['bill_depth_mm'].mean()
                    
                                # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                                palmer_penguins['bill_depth_mm'].fillna(srednia_bill_depth_mm, inplace=True)
                    
                                # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                                srednia_flipper_length_mm   = palmer_penguins['flipper_length_mm'].mean()
                    
                                # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                                palmer_penguins['flipper_length_mm'].fillna(srednia_flipper_length_mm, inplace=True)
                    
                                # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                                srednia_body_mass_g = palmer_penguins['body_mass_g'].mean()
                    
                                # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                                palmer_penguins['body_mass_g'].fillna(srednia_body_mass_g, inplace=True)
                            </code>
                        </pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w
                    tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych
            </p>
            <div class="code-snippet">
                <pre class="line-numbers">
                            <code class="language-python">
                                X = palmer_penguins.drop(["species_Adelie", "species_Chinstrap", "species_Gentoo"], axis=1)  # Wektory cech
                                y = palmer_penguins[["species_Adelie", "species_Chinstrap", "species_Gentoo"]]  # Wektory klas
                                
                                # Podział danych na zbiór treningowy (90%) i testowy (10%)
                                X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
                                
                                # Kolejny podział zbioru X_temp na zbiór walidacyjny (20%) i treningowy (80%)
                                X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)
                                
                                scaler = StandardScaler()
                                
                                # Standaryzacja zbioru treningowego
                                X_train_scaled = scaler.fit_transform(X_train)
                                X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = X_train.columns)
                                
                                # Standaryzacja zbioru walidacyjnego
                                X_valid_scaled = scaler.transform(X_valid)
                                X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns = X_valid.columns)
                                
                                # Standaryzacja zbioru testowego
                                X_test_scaled = scaler.transform(X_test)
                                X_test_scaled_df = pd.DataFrame(X_test_scaled, columns = X_test.columns)
                            </code>
                        </pre>
            </div>
            <p>Teraz wszystkie atrybuty mają wartości przedstawione numerycznie,
                a te, które określają kategorie są zakodowane binarnie.
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci FeedForward</h2>
            <p>Sieć FeedForward (inaczej nazywana sztucznym neuronem, jednokierunkową siecią neuronową lub siecią
                jednokierunkową)
                to jedna z podstawowych architektur sztucznych sieci neuronowych. Jest to rodzaj sieci neuronowej, w której
                informacja
                przemieszcza się tylko w jednym kierunku, od warstwy wejściowej do warstw ukrytych i na koniec do warstwy
                wyjściowej,
                bez żadnych cykli czy sprzężeń zwrotnych.
            </p>

            <div class="plot plot-tiny-width"><img src="img/ff_penguins/siec_feed_forward.webp"></img></div>
            <div class="code-snippet">
                <pre class="line-numbers">
                            <code class="language-python">
                                # Sieć Feed Forward
                                model = Sequential()
            
                                # Dodanie warstw ukrytych
                                model.add(Dense(10, input_dim=X_train_scaled.shape[1], activation='relu'))
            
                                # Dodanie warstwy wyjściowej - zakładamy trzy klasy (species_Adelie, species_Chinstrap, species_Gentoo)
                                model.add(Dense(3, activation='softmax'))
                            </code>
                        </pre>
            </div>
            <p>Struktura sieci:
            <p><code>Warstwa wejściowa (Input Layer):</code></p>
            <p>Zdefiniowana jako input_dim=X_train_scaled.shape[1],
                gdzie X_train_scaled.shape[1] to liczba cech w danych wejściowych po standaryzacji.
                To oznacza, że każdy neuron w tej warstwie odpowiada jednej zmiennej wejściowej w danych.
                Funkcja aktywacji: Brak funkcji aktywacji. Warstwa wejściowa jest liniową reprezentacją danych wejściowych.
            </p>
            <p><code>Pierwsza warstwa ukryta (Hidden Layer 1):</code></p>
            <p>
                Liczba neuronów: 10. Jest to arbitralnie wybrana liczba neuronów, które mają uczyć się nieliniowych zależności w
                danych.
                Funkcja aktywacji: ReLU (Rectified Linear Unit). ReLU jest często używana w warstwach ukrytych i ma postać f(x)
                = max(0, x),
                co oznacza, że jest nieliniowa i pomaga modelowi uczyć się nieliniowych wzorców.
            </p>
            <p><code>Warstwa wyjściowa (Output Layer):</code></p>
            <p>
                Liczba neuronów: 3. Liczba neuronów w warstwie wyjściowej odpowiada liczbie klas lub etykiet, które model ma
                przewidywać (species_Adelie, species_Chinstrap, species_Gentoo).
                Funkcja aktywacji: Softmax. Funkcja softmax jest często używana w warstwie wyjściowej do rozwiązywania problemów
                klasyfikacji wieloklasowej.
                Przekształca wyniki na prawdopodobieństwa przynależności do każdej z klas.
            </p>
            </p>
            <p>Wizualizacja modelu z pomocą biblioteki keras</p>
            <div class="code-snippet">
                <pre class="line-numbers">
                            <code class="language-python">
                                model.summary()
                                keras.utils.plot_model(model, to_file='test.png', show_shapes=True)
                            </code>
                        </pre>
                <pre class="command-line">
            Model: "sequential"
            _________________________________________________________________
            Layer (type) Output Shape Param #
            =================================================================
            dense (Dense) (None, 10) 70
            
            dense_1 (Dense) (None, 3) 33
            
            =================================================================
            Total params: 103 (412.00 Byte)
            Trainable params: 103 (412.00 Byte)
            Non-trainable params: 0 (0.00 Byte)
                        </pre>
            </div>
            <div class="plot plot-small-height"><img src="img/ff_penguins/keras_visualization.png"></img></div>
            <h2>Uczenie sieci</h2>
            <p>Zanim przystąpimy do procesu uczenia się sieci, chcemy najpierw model "skompilować" tzn. skonfigurować go w
                odpowiedni sposób aby był już gotowy do procesu uczenia się. Określimy jak model ma być trenowany i
                jakie metryki zostaną obliczane podczas treningu</p>
            <p><code>loss</code></p>
            <p>Określa funkcję kosztu (ang. loss function), która jest używana do oceny różnicy
                między przewidywaniami modelu a rzeczywistymi etykietami
                (lub wartościami docelowymi). W tym przypadku używamy 'categorical_crossentropy'.
                Funkcja ta jest często stosowana w tego typu problemach.
            </p>

            <p><code>optimizer</code></p>
            <p>Określa algorytm optymalizacji, który będzie używany do aktualizacji wag modelu podczas treningu.
                W przykładzie 'adam' wskazuje na algorytm optymalizacji Adam.
                Adam jest popularnym algorytmem optymalizacji stosowanym w uczeniu maszynowym,
                który adaptacyjnie dostosowuje tempo uczenia w trakcie treningu. </p>

            <p><code>metrics</code></p>
            <p>Określa metryki, które zostaną obliczane podczas treningu i oceny modelu. W przykładzie używamy ['accuracy'],
                co oznacza, że podczas treningu i oceny modelu będzie obliczana dokładność (ang. accuracy), czyli stosunek
                poprawnie
                sklasyfikowanych próbek do wszystkich próbek</p>
            <div class="code-snippet">
                <pre class="line-numbers">
                            <code class="language-python">
                                # Kompilacja modelu
                                model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
                            </code>
                        </pre>
            </div>
            <p>Teraz model jest gotowy do trenowania z użyciem danych treningowych i określonych kryteriów oceny.
                Podczas procesu uczenia funkcja kosztu jest minimalizowana, a wagi modelu są aktualizowane zgodnie z wybranym
                algorytmem optymalizacji.
                Metryki są obliczane, aby monitorować postęp treningu i ocenić wydajność modelu na danych walidacyjnych lub
                testowych.</p>
            <div class="code-snippet">
                <pre class="line-numbers">
                        <code class="language-python">
                            # Trenowanie modelu
                            history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_valid, y_valid))
                        </code>
                    </pre>
            </div>
            <p><code>Parametry</code></p>
            <p>
                1. epochs: Parametr epochs określa liczbę epok treningu.
                Jedna epoka oznacza jedno przejście przez cały zbiór treningowy.
                W tym przypadku ustawienie epochs=10 oznacza, że model będzie trenowany przez 10 epok.
            </p>
            <p>
                2. batch_size: Parametr batch_size określa rozmiar partii danych używanych w jednej iteracji
                treningu.
                Oznacza to, że podczas każdej epoki dane treningowe zostaną podzielone na partie o rozmiarze 10 próbek.
            </p>
            <p><code>Osiągniete rezultaty</code></p>
            <div class="plot plot-small-width"><img src="img/ff_penguins/rezultaty_epoki_text.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/rezultaty_epoki_wykresy.png"></img></div>
            <div class="plot plot-small-height"><img src="img/ff_penguins/confussion_matrix.png"></img></div>
            <div class="plot plot-small-height"><img src="img/ff_penguins/roc_curve.png"></img></div>
            <h2>Dostrajanie parametrów</h2>
            <p>W pierwotnym modelu zostało użyte tylko 10 epok, co jest stosunkowo małą liczbą,
                a dobranie większej liczby może konkretnie zwiększyć nasze rezultaty. Zatem sprawdźmy to</p>
            <div class="plot"><img src="img/ff_penguins/100_epok.png"></img></div>
            <p>Widzimy, że osiągneliśmy 98% dokładności na zbiorze validacyjnym, używając 100 epok.
                Po przebiegu wykresu widzimy natomiast, że liczba ~45 epok daje zadowalające rezultaty
            </p>
            <p>Spróbujmy teraz poeksperymentować z parametrem "batch-size" czyli
                rozmiaru parti używanej w jednej iteracji treningu. W pierwotnym modelu dobraliśmy jego wartość jako 10.
                Poniżej prezentuję rezultaty odpowiedno dla batch size równego 16, 32, 48
            </p>
            <div class="plot"><img src="img/ff_penguins/batch_size_16.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/batch_size_32.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/batch_size_64.png"></img></div>
            <p>Bardzo łatwo zauważyć na wykresach, że im mniejszy batch size tym szybsza jest zbieżność</p>
            <p>Spróbujmy teraz zbadać jeszcze wpływ parametru learning-rate na nasze wyniki.
                W modelu pierwotnym nie podawaliśmy go explicite, ponieważ jest on ustawiany domyślnie na 0.001
                przez algorytm Adam
            </p>
            <p>Learning rate: 0.005</p>
            <div class="plot"><img src="img/ff_penguins/adam_learning_rate_005.png"></img></div>
            <p>Rezultat: Łatwo zauważamy że mamy tutaj przykład "overfittingu"</p>
            <p>Learning rate: 0.01</p>
            <div class="plot"><img src="img/ff_penguins/adam_learning_rate_01.png"></img></div>
            <p>Rezultat jest taki sam jak dla powyższego przykładu,
                mamy tutaj przykład "overfittingu"</p>
            <p>Konkluzja: w naszym ostatecznym modelu, przyjmujemy parametry
                batch size = 10, epoch = 45 oraz learning rate = 0.001 (domyślna wartość)</p>
            <h2>Ocena wyników na dopracowanym modelu i zbiorze testowym</h2>
            <div class="plot"><img src="img/ff_penguins/results_test.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/roc_curve_test_set.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/confussion_matrix_test_set.png"></img></div>
            <p>1. Dokładność mamy na poziomie 100%</p>
            <p>2. Precyzję również mamy na poziomie 100% (widać to łatwo z "Confussion Matrix")</p>
            <p>3. Jeśli chodzi o złożoność naszego wyniku, to ma on tylko 103 parametry,
                a mimo to udało nam się osiągnąć 100% dokładność</p>
        </div>
        

        <!-- STRONA HUBERTA M. -->
        <div class="wrapper page">
            <h2>SOM - Palmer Penguin</h3>
			<div class="plot plot-small-height"><img src="img/penguins_logo.png"></img></div>
			<h2>Opis zbioru danych & Przygotowanie danych</h2>
			<p>Zawarty na stronie Piotra Dziedzica
            <h2>Struktura sieci SOM</h2>
			Zastosowaliśmy bibliotekę <a href="https://github.com/JustGlowing/minisom">Minisom.</a>
			<p>Rozważymy wiele różnych kombinacji ilości atrybutów. Lecz zanim do tego przejdziemy, warto zapoznać się z generalną architekturą sieci:
			<p><b>Architektura sieci:</b><p>
            <ul>
                <li><u>Siatka</u> - 2D, 10x10; - dająca liczbe 100</li>
                <li><u>Liczba neuronów</u> - 100 (zasada heurystyczna dla treningowego zbioru danych
                    o rozmiarze 343 dałaby ~9x10, ale zwiększyliśmy do 10x10 dla ładniejszych wykresów);</li>
                <li><u>Wymiar wektorów wag</u> - 3-6 Zależy od eksperymentu;</li>
                <li><u>Odległość</u> - metryka Euklidesowa</li>
                <li><u>Liczenie sąsiedztwa</u> - metryka Gaussian;</li>
                <li><u>Redukcja współczynników</u> - wykonywana przez minisom.</li>
            </ul>
			
			Startowe wartości wszystkich neuronów są losowane przez minisom.
			
			<p><b>Uczenie sieci & Wyniki:</b><p>
			
			<p>Do utworzenia map dopasowania, SOM jest trenowany 100000 razy.
			<p>Do utworzenia wykresów dokładności korzystamy z osobnych SOM trenowanych na 5 różnych datasetach utworzonych za pomocą walidacji krzyżowej.

			<p>Rozważmy najpierw eksperyment dla wszystkich parametrów.
			<p>Tak jak wcześniej omówiliśmy, odrzucamy kategorie "year" i "island". Omijamy także "species":
			<div class="plot plot-small-width"><img src="img/som_penguins/BaseMap.png"></img></div>
			<p>Widać tutaj klasteryzację, zwłaszcza dla pingwinów reprezentowanych przez niebieskie romby.
			<p>Widać także klaster czerwony i zielony, jednakże te lubią się mieszać.
			<div class="plot plot-small-width"><img src="img/som_penguins/BaseValidation.png"></img></div>
			<p>W obu przypadkach, dokładność tego modelu jest w granicach 63-68%. Jako że dane są szeregowane losowo, te wartości potrafią się zmieniać.
			<p>
			<p>Teraz przetestujmy SOM bez atrybutu "płci":
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexMap.png"></img></div>
			<p>Wygląda na to że nasza mapa się obróciła o 180 stopni. Wciąż widać klastry.
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexValidation.png"></img></div>
			<p>Utrata danych o płci zmniejszyła true positive rate o około 10 punktów procentowych.
			<p>
			<p>Teraz spróbujmy usunąć oba atrybuty o dziobie, zostawiając tylko długość płetwy i masę ciała.
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbillMap.png"></img></div>
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbillValidation.png"></img></div>
			<p>Jak widać, te dwa atrybuty wystarczą aby wciąż widzieć klastry, chociaż są one coraz bardziej rozrzucone. Dodatkowo, nasz SOM jest mniej dodatkowe o kolejne ~10 punktów procentowych.
			<p> 
			<p> Rozważmy teraz coś innego, zamiast usuwać infomacje o dziobie, usuńmy atrybut odpowiadający za masę ciała.
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassMap.png"></img></div>
			<p> Jak widać, teraz wszystkie 3 klastry są bardzo rozdzielone i bardzo widoczne. Nie jest to dziwne, bo pingwiny gatunku Chinstrap i Adelie mają bardzo podobne masy ciał.
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassValidation.png"></img></div>
			<p> Czemu tutaj jest tak mała dokładność chociaż klasty są bardziej widoczne? Bo mniej są trenowane?
			<p>
			<p> Rozważmy jeszcze dwa eksperymenty. Usuńmy dodatkowo oprócz masy ciała, płci jeszcze długość płetwy, która też jest bardzo podobna dla gatunków Chinstap i Adelie:
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassflipperMap.png"></img></div>
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassflipperValidation.png"></img></div>
			<p> Wciąż wyraźnie widać 3 klastry, ale dokładność się nam zmniejszyła do poziomu 36%, to prawie jak losowe zgadywanie!
			<p>
			<p> Jeszcze rozważmy jeden przykład: zamiast usuwać informację o płetwie, rozważmy usunięcie informacji o głębokości dzioba:
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassbilldepthMap.png"></img></div>
			<div class="plot plot-small-width"><img src="img/som_penguins/NosexbodymassbilldepthValidation.png"></img></div>
			<p> Usuwając jeden z wyróżniających atrybutów, znowu klastry się wymieszały. Tym razem zielony i niebieski się mieszają a nasza dokładność jest jeszcze gorsza niż wcześniej.
			<p>
			<p>
			<p><b>Dostrajanie Parametrów:</b><p>
			<p> Za pomocą testów udało nam się wywnioskować że zmiana learning rate z 50% do 90% skutkuje zmniejszeniem dokładności w każdym z eksperymentów. Jest to bardzo zauważalne w eksperymencie bez płci i masy ciała, gdzie % trafionych zgadnięć schodzi z 53% do 36%. Jest jednak jeden wyjątek do tego, albowiem model bez masy ciała i długości płetwy zwiększył swoją dokładność z 36% do 67%.
			<div class="plot plot-small-width"><img src="img/som_penguins/BeforeLearningRate09.png"></img></div>
			<div class="plot plot-small-width"><img src="img/som_penguins/AfterLearningRate09.png"></img></div>
			<p>
			<p> Postanowiliśmy także przetestować zmniejszenie learning rate z 50% do 5%. Tutaj wyniki są o wiele bardziej pozytywne. Każdy z eksperymentów oprócz pierwszego miał zwiększony wskaźnik trafności. Najwiekszą różnicę można zauważyć było w eksperymencie bez atrybutu płci i masy ciała, gdzie trafność zwiększyła się do 73%.
			<div class="plot plot-small-width"><img src="img/som_penguins/AfterLearningRate005.png"></img></div>
			<p>
			<p> Spróbowaliśmy zwiększyć rozmiar mapy do 20x20, ale to poskutkowało zmniejszeniem trafności do 28-36% w każdym z przypadków.  
        </div>



        <!-- STRONA HUBERTA P. -->
        <div class="wrapper page">
            <h2>FFNN - Heart Disease</h2>
            <p>Nagłówki zostaną po angielsku dla spójności z <a href="https://drive.google.com/file/d/1qiDcnclMQwPquOmphOMx-5t5TrzHximV/view?usp=sharing">notatnikiem</a>, który jest cały po angielsku (na którego to podstawie jest ten artykuł).</p>
            <h2>Table of contents</h2>
            <ol>
                <li>Dataset description</li>
                <li>Data preparation</li>
                <li>Division into training and test sets</li>
                <li>Network structure</li>
                <li>Network training</li>
                <li>Results</li>
                <li>Results analysis</li>
                <li>Parameter tuning</li>
            </ol>
            <h2>Dataset description</h2>
            <p>Dokładny opis zbioru danych został wykonany na podstronie Taira Yerniyazova w związku z czym tu przedstawię to skrótowo.</p>
            Najpierw potrzebne biblioteki oraz zbiór danych.
            Wykorzystam ucimlrepo do łatwego importowania zestawów danych z UC Irvine Machine Learning Repository.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    !pip install ucimlrepo
                    from ucimlrepo import fetch_ucirepo
                    import pandas as pd
                    import seaborn as sns
                    from sklearn.model_selection import train_test_split
                    from sklearn.preprocessing import normalize
                    from matplotlib import pyplot as plt
                    import tensorflow as tf
                    from tensorflow.keras.models import Sequential
                    from tensorflow.keras.layers import Dense, Dropout
                    from sklearn.metrics import roc_curve, auc
                    from IPython.display import Javascript
                    plt.interactive(False)
                    plt.style.use('dark_background')

                    heart_diseases = fetch_ucirepo(id=45)
                    X = heart_diseases.data.features # big letter for matrix
                    y = heart_diseases.data.targets # small letter for vector
                    X.isna().sum()
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    age         0
                    sex         0
                    cp          0
                    trestbps    0
                    chol        0
                    fbs         0
                    restecg     0
                    thalach     0
                    exang       0
                    oldpeak     0
                    slope       0
                    ca          4
                    thal        2
                    dtype: int64
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    plt.figure(figsize=(10, 8))
                    sns.heatmap(heart_diseases.data.original.corr(), annot=True, cmap='coolwarm', fmt=".2f")
                    plt.title("Correlation matrix")
                    plt.show()
                </code></pre>
                <div class="plot"><img src="img/ff_hd/corr_matrix.png"></img></div>
            </div>
            <h2>Data preparation</h2>
            <p>Brakujących wartości było tylko kilka, dlatego je wypełniamy. Nie chcemy wyrzucać wartości, bo zbiór jest mały (303 instancje).</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    for column in X.isna().sum()[X.isna().sum() > 0].index.values:
                        X[column].fillna(X[column].median(), inplace=True)
                </code></pre>
            </div>
            <p>Następnie dane kategoryczne rozdzielamy na tyle column ile jest kategorii i w danym wierszu wstawaimy 1 w jedną z kolumn, zaś w pozostałe 0. Wymaga tego nasza sieć do poprawnej interpretacji danych.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    dummy_X = pd.get_dummies(X, columns=heart_diseases.variables[heart_diseases.variables['type'] == 'Categorical']['name'].values)
                </code></pre>
            </div>
            <p>Na końcu normalizujemy dane, aby wszystkie były dla sieci tak samo ważne. Wywołanie funkcji normalize dla zbioru y, w którym pierwotnie są liczby ze zbioru {0, 1, 2, 3, 4}, spowoduje, że 0 zostaną 0, a pozostałę liczby staną się 1. Wynika to ze specyfikacji zbioru danych. Autorzy nie rozróżniają liczb większych od 1.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    np_X = normalize(dummy_X.values, axis=0)
                    np_y = normalize(y.values)
                </code></pre>
            </div>
            <h2>Division into training and test sets</h2>
            <p>Cały zbiór dzielę na razie tylko na zbiór treningowy i testowy. Testowe 15% zbioru zostanie użyte dopiero na samym końcu po dobraniu hiperparametrów, żeby ocenić jak sieć sobie poradzi na danych, których nie widziała ani ona, ani programista dostosowywujący hiperparametry. Do dostosowywania hiperparametrów będziemy używali czegoś zbliżonego do cross-validation, tzn. za każdym razem do ewaluacji widocznej na późniejszych wykresach będzie używane 15% ze zbioru treningowego, zaś zbiorem, na którym sieć się rzeczywiście będzie trenować będzie 70% pierwotnego zbioru. Jednak wybierany zbiór 15% do walidacji będzie losowany dla każdego treningu, dzięki czemu sieć będzie się mogła uśredniając trenować dla 85% pierwotnego zbioru.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X_train, X_test, y_train, y_test = train_test_split(np_X, np_y, test_size=.15, random_state=1)
                    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(.15/.85))
                    print(X_train.shape, X_test.shape) # , X_val.shape
                    print(y_train.shape, y_test.shape) # , y_val.shape
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    (257, 25) (46, 25)
                    (257, 1) (46, 1)
                </pre>
            </div>
            <h2>Network structure</h2>
            <p>Sieć będzie mała, ponieważ po pierwsze zbiór danych jest mały, a po drugie rozwiązywanym problemem jest klasyfikacja binarna. Stosujemy prosty (bo do wyboru mamy jeszcze inne, bardziej skomplikowane i samoobsługowe) Stochastic Gradient Descent, którego działanie było omawiane na przedmiocie Metody Numeryczne. Wartości hiperparametrów takich jak learning_rate i momentum, wynikają na tym etapie (od jakichś trzeba zacząć) z empirycznych obserwacji, że dla wielu modeli stosujących SGD te były optymalne.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    units1 = 20
                    units2 = 10
                    units3 = 1
                    model = Sequential([
                        Dense(units1, activation='relu', input_shape=(X_train.shape[1],)),
                        Dense(units2, activation='relu'),
                        # Dropout(0.2),
                        Dense(units3, activation='sigmoid')
                    ])
                    model.compile(optimizer=tf.keras.optimizers.SGD(
                        learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
                </code></pre>
            </div>
            <h2>Network training</h2>
            <p>Funkcja train_test_split wybiera losowe lekko ponad 15% zbioru treningowego (stanowiące 15% całego zbioru pierwotnego). Domyślna wartość parametru batch_size funkcji model.fit wynosi 32, zaś domyślnie epochs = 1, dlatego przyjąłem na dobry początek 100.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X, Xv, y, yv = train_test_split(X_train, y_train, test_size=(.15/.85))
                    history = model.fit(X, y, epochs=100, validation_data=(Xv, yv), verbose=0)
                </code></pre>
            </div>
            <h2>Results</h2>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    plt.figure(figsize=(15, 5))

                    plt.subplot(1, 3, 1)
                    plt.plot(history.history['loss'], label='Training Loss')
                    plt.plot(history.history['val_loss'], label='Validation Loss')
                    plt.title('Loss on Training and Validation Data')
                    plt.xlabel('Epochs')
                    plt.ylabel('Loss')
                    plt.legend()

                    plt.subplot(1, 3, 2)
                    plt.plot(history.history['accuracy'], label='Training Accuracy')
                    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
                    plt.title('Accuracy on Training and Validation Data')
                    plt.xlabel('Epochs')
                    plt.ylabel('Accuracy')
                    plt.legend()

                    # false positive rate (fpr), true positive rate (tpr)
                    fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))
                    roc_auc = auc(fpr, tpr) # Area Under Curve (AUC)
                    plt.subplot(1, 3, 3)
                    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
                    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
                    plt.xlabel('False Positive Rate')
                    plt.ylabel('True Positive Rate')
                    plt.title('Receiver Operating Characteristic (ROC) Curve')
                    plt.legend(loc='lower right')

                    plt.tight_layout()
                    plt.show()
                </code></pre>
            </div>
            <div class="plot"><img src="img/ff_hd/first_network.png"></img></div>
            <p>Widocznie mieliśmy szczęście z domyślnymi hiperparametrami, ponieważ już teraz sieć rozpoznaje około 85% przypadków. Poniżej jest zdjęcie ze strony zbioru, dzięki czemu teraz i na końcu będziemy mogli porównać nasz model z tym, co jest uważane za przedział dokładności dla klasyfikacyjnej sieci neuronowej.</p>
            <div class="plot"><img src="img/ff_hd/baseline_model_performance.png"></img></div>
            <h2>Results analysis</h2>
            <p>Z pierwszego treningu wynika, że sieć się nie douczyła, ponieważ Training Loss > Validation Loss oraz Training Accuracy < Validation Accuracy. Jednakże od około połowy epochs widzimy więcej fluktuacji, niż trendu zmian w związku z czym może trzeba będzie poprawić np. batch_size lub learning_rate.</p>
            <h2>Parameter tuning</h2>
            <p>Przygotowaliśmy funkcję do testowania zmian hiperparametrów. Jako argumenty przyjmuje rozkład wykresów, zbiór treningowy, parametr, który będziemy zmieniać oraz zakres zmian. Opcjonalnymi argumentami są mnożnik zmian (jak skalować od do oraz krok) oraz czy model ma zawierać dropout. Gdy będziemy chcieli zmienić hiperparametr na stałe, dokonamy tego wewnątrz tej funkcji. Zaczynamy z batch_size = 32, epochs = 100, learning_rate = 0.01, momentum = 0.9. Bardzo przydatne są linijki print'ujące progres testu, bo czasem można stracić cierpliwość, gdy program już długo działa i nie wiadomo jak długo trzeba będzie jeszcze czekać.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    def default_model():
                        return Sequential([
                            Dense(units1, activation='relu', input_shape=(X_train.shape[1],)),
                            Dense(units2, activation='relu'),
                            Dense(units3, activation='sigmoid')
                        ])
                    def dropout_model(dropout=0.2):
                        return Sequential([
                            Dense(units1, activation='relu', input_shape=(X_train.shape[1],)),
                            Dense(units2, activation='relu'),
                            Dropout(dropout),
                            Dense(units3, activation='sigmoid')
                        ])
                    def compile_model(model, learning_rate, momentum):
                        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum), loss='binary_crossentropy', metrics=['accuracy'])
                    def fit_model(model, X_train, y_train, batch_size, epochs):
                        model.fit(X_train, y_train, batch_size, epochs, verbose=0) #validation_data=(X_val, y_val)
                    def plot_models(subplot, X_train, y_train, change, max, multiplier=1, min=0, model_type="Without dropout", title=""):
                        step = int((max-min)/50)     # Here is the magic place where we set how long program will work
                        if(step<1): step = 1
                        batch_size = 32
                        epochs = 100
                        learning_rate = 0.01
                        momentum = 0.9
                        if(model_type == "With dropout"): model = dropout_model()
                        else: model = default_model()
                        compile_model(model, learning_rate, momentum)
                        validation_losses = []
                        validation_accuracies = []
                        for i in range(min, max+1, step):
                            if "batch_size" in change: batch_size = multiplier*i
                            if "epoch" in change: epochs = multiplier*i
                            if "learning_rate" in change: compile_model(model, multiplier*i, momentum)
                            if "momentum" in change: compile_model(model, learning_rate, multiplier*momentum)
                            X, Xv, y, yv = train_test_split(X_train, y_train, test_size=(.15/.85))
                            fit_model(model, X, y, batch_size=batch_size, epochs=epochs)
                            print("Done "+str(((subplot[2]-2)/(subplot[0]*subplot[1])+(i/max+1)/(subplot[0]*subplot[1]))*100)
                            +"% ("+str(i)+"/"+str(max+1)+" from "+str(subplot)+")")
                            # X_test, y_test instead of Xv, yv only when nothing will be changed later
                            test_loss, test_accuracy = model.evaluate(Xv, yv)
                            validation_losses.append(test_loss)
                            validation_accuracies.append(test_accuracy)

                        ax1 = plt.subplot(subplot[0], subplot[1], subplot[2])
                        ax1.plot([multiplier*i for i in range(min, max+1, step)], validation_accuracies, color="#198038")
                        ax1.set_ylabel('Validation accuracy', color="#198038")
                        ax2 = ax1.twinx()
                        ax2.plot([multiplier*i for i in range(min, max+1, step)], validation_losses, color="#1192e8")
                        ax2.set_ylabel('Validation loss', color="#1192e8")
                        plt.title(str(model_type)+" "+str(title))
                        plt.xlabel(" ".join([str(item) for item in change]))
                </code></pre>
            </div>
            <h3>Number of epochs</h3>
            <p>Powyżej 100 epoc nie widać poprawy, więc epochs zastawiamy 100.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))
                    plt.figure(figsize=(10, 4))
                    plot_models([1,2,1], X_train, y_train, "epoch", 400)
                    plot_models([1,2,2], X_train, y_train, "epoch", 400, model_type="With dropout")
                    plt.tight_layout()
                    plt.show()
                </code></pre>
            </div>
            <div class="plot"><img src="img/ff_hd/epochs.png"></img></div>
            <h3>Batch size</h3>
            <p>W przypadku batch_size również na razie nie widać, żeby zmiana z domyślnej wartości 32 miała sens.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))
                    plt.figure(figsize=(10, 4))
                    plot_models([1,2,1], X_train, y_train, "batch_size", 100)
                    plot_models([1,2,2], X_train, y_train, "batch_size", 100, model_type="With dropout")
                    plt.tight_layout()
                    plt.show()
                </code></pre>
            </div>
            <div class="plot"><img src="img/ff_hd/batch_size.png"></img></div>
            <h3>Learning rate</h3>
            <p>Fluktuacje sugerują dystans do tego jaka wartość jest ściśle najlepsza, jednak decydujemy się na zwiększenie learning_rate z 0.01 na 0.05.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))
                    plt.figure(figsize=(10, 4))
                    plot_models([1,2,1], X_train, y_train, "learning_rate", 100, multiplier=0.001)
                    plot_models([1,2,2], X_train, y_train, "learning_rate", 100, multiplier=0.001, model_type="With dropout")
                    plt.tight_layout()
                    plt.show()
                </code></pre>
            </div>
            <div class="plot"><img src="img/ff_hd/learning_rate.png"></img></div>
            <h3>Momentum</h3>
            <p>Fluktuacje są bardzo duże, zaś accuracy jest bardzo niskie, średnio rzędu 55%, dlatego zmieniamy learning_rate spowrotem z 0.05 na 0.01 i uruchamiamy test momentum jeszcze raz.</p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))
                    plt.figure(figsize=(10, 4))
                    plot_models([1,2,1], X_train, y_train, "momentum", 100, multiplier=0.01)
                    plot_models([1,2,2], X_train, y_train, "momentum", 100, multiplier=0.01, model_type="With dropout")
                    plt.tight_layout()
                    plt.show()
                </code></pre>
            </div>
            <div class="plot"><img src="img/ff_hd/momentum_1.png"></img></div>
            <p>Fluktuacje są bardzo duże, zaś accuracy jest bardzo niskie, średnio rzędu 55%, dlatego zmieniamy learning_rate spowrotem z 0.05 na 0.01 i uruchamiamy test momentum jeszcze raz.</p>
            <div class="plot"><img src="img/ff_hd/momentum_2.png"></img></div>
            <h2>Example of a good model</h2>
            p></p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    
                </code></pre>
            </div>

            <!-- Template:
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    
                </pre>
                <div class="plot"><img src="img/ff_hd/corr_matrix.png"></img></div>
            </div> -->
        </div>


        <!-- STRONA TAIRA Y. -->
        <div class="wrapper page">
            <h2>Opis zbioru danych</h2>
            <p>Zbiór danych <b><a href="https://archive.ics.uci.edu/dataset/45/heart+disease">
            "Heart Disease (Cleveland Clinic)"</a></b> dotyczy pacjentów, część z których ma chorobę serca.
            Mamy do dyspozycji 303 instancje zawierające 14 atrybutów numerycznych i kategorialnych.
            W trakcie dokonywania krótkiej analizy (<a href='https://colab.research.google.com/drive/1dRjVmMjxnA88s9SXnwcu2DvbYx6zUVKu?usp=sharing'>notatnik Jupyter, Google Colab)</a> 
            podczas pracy naszego zespołu nad pierwszą częścią
            projektu zauważyliśmy dosyć nierównomierny rozkład poszczególnych atrybutów. Na szczęście
            dla wielu kolumn mamy rozkłady zbliżone do normalnych, więc po standaryzacji danych cały zbiór
            będzie wyglądał sensownie z punktu widzenia trenowania sieci. 
            <div class="row-with-images">
                <div><img src="img/som_hd/dataset_distribution_1.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_2.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_3.png"></img></div>
            </div>
            Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
            uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
            przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
            posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
            (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
            odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Pobranie repozytorium
                    !pip install ucimlrepo

                    # Zapis danych w postaci pandas dataframes
                    X = heart_diseases.data.features
                    y = heart_diseases.data.targets

                    # Wyliczenie macierzy korelacji
                    corr_matrix = heart_diseases.corr()
                    corr_matrix["num"].sort_values(ascending=False)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    num         1.000000
    oldpeak     0.504092
    cp          0.407075
    exang       0.397057
    slope       0.377957
    sex         0.224469
    age         0.222853
    restecg     0.183696
    trestbps    0.157754
    chol        0.070909
    fbs         0.059186
    thalach     -0.415040
    Name: num, dtype: float64
                </pre>
            </div>
            <p>Złą wiadomością jest to, że zbiór, który i tak zawiera za dużo atrybutów i za małą
                liczbę instancji, posiada rekordy cechujące się obecnością brakujących wartości
                w poszczególnych kolumnach (<code>ca</code> oraz <code>thal</code>). Mamy 2 rekordy z
                wartościami <code>NaN</code> (Not a Number) w kolumnie <code>ca</code> oraz 4 rekordy, które
                są wypełniony tymi samymi pseudo-wartościami w kolumnie <code>thal</code>.
            </p>
            <p>Dobrą wiadomością jest stosunek liczby osób chorych do zdrowych. Mamy
                prawie że idealne rozbicie na <b>2 połówki (54% zdrowych i 46% chorych)</b>, więc
                nie musimy zastanawiać się nad metodami redukcji uprzedzeń występujących w zbiorze
                danych treningowych (idealnie oczywiście jest mieć w pełnie równomierny rozkład
                wartości przewidywanych w klasyfikacji binarnej, gdyż łatwiej będzie określić
                najniższy próg skutecznej dokładności).
            </p>
            <p>Rozważmy teraz opis każdego z atrybutów danych wejściowych, które zostały dołączone
                do plików źródłowych autorów tego zbioru, gdyż w kolejnym etapie nam to się przyda:
            </p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
                <ul>
                    <li><u>age</u> - wiek;</li>
                    <li><u>trestbps</u> - spoczynkowe ciśnienie krwi;</li>
                    <li><u>chol</u> - cholesterol w surowicy</li>
                    <li><u>thalach</u> - osiągnięte maksymalne tętno;</li>
                    <li><u>oldpeak</u>- obniżenie odcinka ST wywołane wysiłkiem fizycznym w 
                        stosunku do odpoczynku;</li>
                    <li><u>cp</u> - liczba głównych naczyń (0-3) pokolorowanych metodą 
                        fluorosopii.</li>
                </ul>

                <b>Kategorialne biarne</b>:
                <ul>
                    <li><u>sex</u> - płeć;</li>
                    <li><u>fbs</u> - cukier we krwi na czczo;</li>
                    <li><u>exang</u> - dławica piersiowa wywołana wysiłkiem fizycznym.</li>
                </ul>

                <b>Kategorialne uporządkowane</b>:
                <ul>
                    <li><u>restecq</u> - spoczynkowe wyniki elektrokardiograficzne 
                        (0 - norma, 1 - anomalia, 2 - hipertrofia);</li>
                    <li><u>slope</u> - nachylenie szczytowego odcinka ST podczas wysiłku 
                        (0 - wznoszące się, 2 - płaskie, 3 - opadające).</li>
                </ul>
    
                <b>Stricte kategorialne</b>:
                <ul>
                    <li><u>cp</u> - rodzaj bólu w klatce piersiowej (cztery typy);</li>
                    <li><u>thal</u> = niedokrwistość tarczowatokrwinkowa (trzy typy).</li>
                </ul>
            </p>
            <p>Oczywiscie, niektóre atrybute są bardziej istotne, inne mniej. Teraz
                nie możemy stwierdzać nic poza określeniem współczynnika korelacji.
                Jeśli połączymy razem wartości opisujące wiek oraz puls pacjenta, uzyskamy nietrywialny wykres,
                na którym dobrze widać medianę próbek względem stosunku tych liczb:
            </p>
            <div class="plot"><img src="img/som_hd/correlation_1.png"></img></div>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="https://colab.research.google.com/drive/1aFp_SQcUAk8FIX2B24jQNYbMutnl50P_?usp=sharing">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Przede wszystkim powinniśmy jakoś postąpić z brakującymi wartościami. Ze zwględu na to,
                że każdy rekord zawiera 13 cech, to możemy te dwie znane nam kolumny douzupełnić wartością
                średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                303 instancje, ani nie stać nas na wyrzucenie całych tych kolumn, gdyż już na etapie
                sprawdzenie zależności liniowych (korelacji) z atrybutem przewidywanym wykazały się
                dosyć poważnie. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Rzutowanie wszystkich kolumn na liczby zmiennoprzecinkowe
                    X = X.astype("float")

                    # Imputacja wartości
                    from sklearn.impute import SimpleImputer
                    import numpy as np
                    
                    imputer = SimpleImputer(missing_values = np.NaN, strategy="median")
                    imputer.fit(X[['ca', 'thal']])
                    imputed_X_columns = imputer.transform(X[['ca', 'thal']])
                    
                    X.loc[:, ('ca', 'thal')] = imputed_X_columns
                </code></pre>
            </div>
            <p>Kolejna rzecz, którą musimy załatwić, polega na przerzuceniu wartości przewidywanego
                atrybutu na liczby 0 i 1, gdyż wszystko powyżej 0 wskazuje na obecność choroby i nie 
                dodaje żadnego innego znaczenia. Zróbmy to teraz:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Zamiana przewidywanego atrybutu na wartości binarne zgodnie z opisem zbioru
                    y.loc[y['num'] > 0, 'num'] = 1.0
                </code></pre>
            </div>
            </p>Następnie na naszej liście zadań do zrobienia mamy oczywiście przetworzenie atrybutów
            kategorialnych. Tak jak wspomnieliśmy wcześniej, niektóre z nich są binarne, więc z nimi
            nie będzie żadnego problemu, gdyż mieszczą się w przedziale od 0 do 1. Inne mają porządek
            semantyczny i określają stopień konkretnej cechy, więc też je możemy bezpiecznie zostawić, a
            później standaryzować jako atrybuty numeryczne. Zostaje nam problem tych atrybutów, które
            są stricte kategorialne, tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
            konkretnych cech.
            <p>Możemy te atrybutu (<code>cp</code> oraz <code>thal</code>) rozbić na kilka kolumn.
            Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
            Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
            (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który to robi
            dla atrybutu <code>cp</code>:
            </p> 
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X_cp = X[['cp']]
                    
                    # Zakodujmy najpierw "gorącojedynkowo" wszystkie cztery typy 'cp'
                    from sklearn.preprocessing import OneHotEncoder
                    
                    cat_encoder = OneHotEncoder()
                    X_cp_1hot = cat_encoder.fit_transform(X_cp)

                    # Teraz mamy dodatkowe 4 kolumny (każda odpowiada za obecność 
                    # określonego rodzaju bólu w klatce piersiowej):
                    print(X_cp_1hot.toarray())
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
   [[1. 0. 0. 0.]
    [0. 0. 0. 1.]
    [0. 0. 0. 1.]
    ...
    [0. 0. 0. 1.]
    [0. 1. 0. 0.]
    [0. 0. 1. 0.]]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Pierwotne etykiety (numery typu bólu, 1-4) mamy wciąż pod ręką
                    cat_encoder.categories_
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    [array([1., 2., 3., 4.])]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Dodajmy teraz te typy zakodowane binarnie do naszych danych i wyrzućmy kolumnę 'cp'
                    for i in range(4):
                        X[f"cp_type_{i + 1}"] = X_cp_1hot.toarray()[:, i].reshape(-1, 1)
                    X.drop(columns=["cp"], inplace=True)
                </code></pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w 
                tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych (oprócz atrybutów binarnych):
            <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                from sklearn.model_selection import train_test_split
        
                # Podział na zbióry: testowy, treningowy
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)
                
                # Standaryzacja
                from sklearn.preprocessing import StandardScaler

                num_columns = ['age', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca']
                cat_columns = ['sex', 'fbs', 'exang', 'cp_type_1', 'cp_type_2', 'cp_type_3', 'cp_type_4', 'thal_3', 'thal_6', 'thal_7']
                X_train_cat = X_train.loc[:, cat_columns]
                X_train_num = X_train.loc[:, num_columns]

                std_scaler = StandardScaler()
                X_train_num_scaled = std_scaler.fit_transform(X_train_num)
                X_train = np.concatenate((X_train_num_scaled, X_train_cat.to_numpy()), axis=1)
                
                y_train = y_train.to_numpy()
            </code></pre></div>
            <p>Jeśli chodzi o dane do walidacji, to sensownym rozwiązaniem wydaje się być
                stosowanie K-składowej walidacji krzyżowej (z ang. <b>K-Fold Cross Validation</b>):
                <div class="plot"><img src="img/som_hd/k_fold_validation.png"></img></div>
                <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                    k_folds = 5
                    num_val_samples = len(X_train) // k_folds
                    num_last_val_samples = len(X_train) % k_folds
                    
                    X_validation_sets = []
                    y_validation_sets = []
                    
                    X_train_sets = []
                    y_train_sets = []
                    
                    for i in range(k_folds):
                        print('Processing fold #', i)
                        X_validation_sets.append(X_train[i * num_val_samples: (i + 1) * num_val_samples])
                        y_validation_sets.append(y_train[i * num_val_samples: (i + 1) * num_val_samples])
                        X_train_sets.append(np.concatenate([X_train[:i * num_val_samples],
                        X_train[(i + 1) * num_val_samples:]], 
                        axis=0))
                        y_train_sets.append(np.concatenate([y_train[:i * num_val_samples],
                        y_train[(i + 1) * num_val_samples:]],
                        axis=0))
                    
                        # Dodajemy resztę do ostaniego zbioru walidacyjnego
                        if i == (k_folds - 1):
                        X_validation_sets[k_folds - 1] = np.concatenate((X_validation_sets[k_folds - 1], X_train[- num_last_val_samples:]),
                        axis=0)
                        y_validation_sets[k_folds - 1] = np.concatenate((y_validation_sets[k_folds - 1], y_train[- num_last_val_samples:]),
                        axis=0)
                </code></pre></div>
            <p>Teraz bodajże wszystkie atrybuty mają wartości przedstawione numerycznie, 
                a te, które określają kategorie, mają albo porządek, albo są zakodowane binarnie. 
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci SOM</h2>
            <p>Wiemy, oczywiście, że mamy do czynienia z problemem klasyfikacji, więc moglibyśmy 
                od razu w przestrzeni neuronów ustawić tylko dwie jednostki (jedna odpowiadałaby
                wówczas za osobe chore, inny za zdrowe). Jednak nie będzie to
                prawdziwa SOM, tylko zwyczajne wykonanie algorytmu LVQ, stosując uczenie nadzorowane.
        
            <p>Spróbujmy zatem postąpić inaczej. Wyobraźmy sobie, że nie wiemy, że mamy dane naszych 
                pacjęntów i nie wiemy, czy wgl oni się wyodrębniają w jakieś osobne kategorie czy nie. 
                Wtedy mamy uczenie nienadzorowane, więc klastry "osoby chore" i "osoby zdrowe" 
                powstaną naturalnie (co najmniej w teorii, bo możliwe, że siec nam zamiast tego np.
                rozdzieli ludzi na kobiet i mężczyzn, ale to jest mniej prawdopodobne, bo 17 kolumn 
                wskazują raczej na to, że płeć nie stanowi za dużej różnicy w przypadku danych
                medycznych dotyczących chorób serca).

            <p><b>Architektura sieci:</b><p>
            <ul>
                <li><u>Siatka</u> - 2D, 9x9;</li>
                <li><u>Liczba neuronów</u> - 81 (zasada heurystyczna dla treningowego zbioru danych
                    o rozmiarze 272 rekordy);</li>
                <li><u>Wymiar wektorów wag</u> - 18 (co odpowiada liczbie atrybutów);</li>
                <li><u>Odległość</u> - metryka Euklidesowa;</li>
                <li><u>Liczenie sąsiedztwa</u> - metryka Manhattan;</li>
                <li><u>Redukcja współczynników</u> - zależy od maksymalnych wartości oraz całkowitej
                liczby iteracji.</li>
            </ul>
            <div class="plot"><img src="img/som_hd/som_nn.png"></img></div>
            <p>Skoro mamy w zbiorze treningowym 272 atrybuty, to zgodnie z zasadą kciuka, która mówi, że
                liczba neuronów to 5 * sqrt(N), lepiej jest tworzyć siatkę 9x9, co właśnie zrobiliśmy. 
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Inicjalizacja parametrów siatki
                    n_dimensions = 18
                    grid_size = 9
                    
                    # Inicjalizacja wag neuronow (do klasteryzacji)
                    initial_weights = np.zeros((grid_size, grid_size, n_dimensions))
                    for i in range(grid_size):
                    for j in range(grid_size):
                    for k in range(n_dimensions):
                    initial_weights[i, j, k] = np.random.uniform(-1, 1, 1)
                </code></pre>
            </div>
            <p>Metryka Manhattan pozwala na szybkie i proste liczenie promienia obszaru
                sąsiedztwa, gdyż jest to zwykła norma L1 (Lasso).
            </p>
            <h2>Uczenie sieci</h2>
                <p>Algorytmów do wyboru w przypadku sieci SOM nie mamy za dużo. Z definicji sieci
                    jest to tzw. Online learning. Czyli dokładamy do siatki neuronów o losowo zainincjalizowanych
                    wagach kolejne próbki i wyłuskujemy neuron-zwycięzcę (mierzymy to za pomocą metryki
                    Euklidesowej: (przypisane wagi neuronu, atrybuty/kolumny próbki)). Może jedyna rzecz,
                    na którą warto zwrócić uwagę, jest taka że, że obszar sąsiedztwa operuje nie na przestrzeni wag
                    neuronów (de facto przestrzeni danych wejściowych), tylko na samej siatce, która jest 
                    statyczna, czyli położenie neuronów się nigdy nie zmienia. Na wielu wizualizacjach dostępnych
                    w Internecie często pokazuje się, jak siatka ta rozciąga się i przyjmuje kształt danych wejściowych.
                    Oczywiście w taki sposób należy rozumieć zmianę/dostrajanie wag. Natomiast tak wizualnie, to jest
                    raczej na odwrót. To próbki są rozpinane na siatce i rzucane w taki sposób, że każdy rekord z X znajdzie
                    swój neuron w Y. Później patrzymy na siatkę i widzimy (co najmniej mamy taką nadzieję) klastry.
                </p>
                <p><b>Wybrane parametry:</b>
                <p>
                <ul>
                    <li><u>Learning rate</u> - 0.9;</li>
                    <li><u>Maksymalny promień obszaru sąsiedztwa</u> - 10;</li>
                    <li><u>Liczba iteracji ("epoki" * liczba próbek)</u> - 6528;</li>
                </ul>
                <div class="code-snippet">
                    <pre class="line-numbers"><code class="language-python">
                        def train_SOM(n_epochs, X_part, y_part, weights, n_dimensions, grid_size, show_plots=False,
                                    progress_bar=False, max_lr=.9, max_ds=10):
                            # Hiperparametry modelu
                            max_learning_rate = max_lr
                            learning_rate = max_learning_rate

                            max_distance = max_ds
                            dist = max_distance

                            epochs = n_epochs
                            n_samples = len(X_part)

                            total_n_steps = epochs * n_samples
                            step = 0

                            if show_plots:
                                plot_U_matrix(step, weights, grid_size)
                                plot_labels(step, grid_size, weights, X_part, y_part)
                                plot_labels(step, grid_size, weights, X_part, y_part, majority_voting=True)

                            # Trenowanie sieci SOM
                            for _ in range(epochs):
                                # Iterowanie po wszystkich próbkach z przestrzeni wejściowej
                                for sample_index in range(n_samples):

                                # Aktualizacja współczynnika uczenia się oraz dystansu funkcji sąsiedzstwa
                                learning_rate, dist = decay(step, total_n_steps, max_learning_rate, max_distance)
                                sample = X_part[sample_index]

                                # Liczenie odległości od wszystkich neuronów
                                distances = [[euclidian_distance(sample, weights[i, j]) for j in range(grid_size)] for i in range(grid_size)]

                                # Znalezienie neuronu-zwycięzcy
                                winner_index = np.argmin(distances)

                                # Wyłuskanie indeksu zwycięzcy
                                row = winner_index // grid_size
                                col = winner_index % grid_size

                                # Aktualizacja wag wszystkich neuronów
                                for j in range(grid_size):
                                for i in range(grid_size):
                                if man([row, col], [i, j]) <= dist: weights[i, j]=weights[i, j] + learning_rate * (sample - weights[i, j]) 
                                
                                # Rysowanie wykresów 
                                if step % 1000==0: 
                                    if progress_bar: 
                                        sys.stdout.write(f"\rStep {step + 1}\n") 
                                    if show_plots: 
                                        if step % 1000 == 0: 
                                            plot_U_matrix(step + 1, weights, grid_size) 
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part)
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part, majority_voting=True) 
                               
                                # Zliczanie liczby iteracji
                                step=step + 1 
                        
                        # Zwrócenie macierzy, którą będziemy mogli wykorzystać do predykcji 
                        return np.copy(weights),
                        get_prediction_matrix(grid_size, weights, X_part, y_part, majority_voting=True)
                    </code></pre>
                </div>
                    <p>Oczywiście dostrajaliśmy parametry, o czym powiemy później. W trakcie trenowania odwoływaliśmy się
                        do własnych zdefiniowanych funkcji, które rysowały nam odpowiednie wykresy. Najpierw
                        po każdej iteracji kolorowaliśmy stany neuronów (czerwony - trafił chory pacjent, niebieski - 
                        trafił zdrowy pacjent, szary - jeszcze nikt nie trafił i wagi neuronu nie są zmieniane):
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/1.png"></img></div>
                        <div><img src="img/som_hd/2.png"></img></div>
                    </div>
                    <p>Możemy też zobaczyć jak wygląda macierz odległości (U-Matrix). Mapa odległości (U-Matrix) 
                        pokazuje nam odległość pomiędzy wagami sąsiednich neuronów, a to właśnie te wagi dostrajane
                    są do kształtu danych z przestrzeni wejściowej. Zatem im dalej dwa neurony od siebie się znajdują w sensie "wagowym",
                    tym bardziej różniący się od siebie próbki te neurony chwyciły. Na mapie podczas klasteryzacji tworzą się ściany, które
                    separują nam zgrupowane rekordy.</p> 
                    <div class="row-with-images">
                        <div><img src="img/som_hd/3.png"></img></div>
                        <div><img src="img/som_hd/4.png"></img></div>
                    </div>
                    <p>Żeby móc klasyfikować próbki ze zbioru walidacyjnego, użyliśmy tak zwanego Majority Voting, czyli głosowania.
                        Innymi słowy, jeśli do neuronu trafiają 4 czerwone punkty i 8 niebieskich, to uznajemy, że ten neuron nadaje
                        nowym próbkom etykietę "osoba zdrowa". Jeżeli mamy w głosowaniu niejednoznaczność albo neuron w ogóle
                        nie złapał żadnego punktu podczas trenowania, to oznaczamy na szaro taki neuron jako niezainicjalizowany.
                        Podczas klasyfikacji zachowujemy wtedy stabilność statystyczną i próbujemy zgadnąć (czyli prawdopodobieństwo = 50%).
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/5.png"></img></div>
                        <div><img src="img/som_hd/6.png"></img></div>
                    </div>
                    <h2>Wyniki</h2>
                    <p>Możemy zobaczyć końcowy wynik (po wcześniejszej walidacji, oczywiście, którą przedstawimy w kolejnej części):</p>
                    <div class="code-snippet">
                        <pre class="line-numbers"><code class="language-python">
                            from sklearn.metrics import accuracy_score

                            # Końcowa ewaluacja na zbiorze testowym
                            ground_truth_values = y_test
                            predictions = []
                            
                            for sample_index in range(len(X_test)):
                                predictions.append(predict_sample(X_test[sample_index], prediction_matrix, weights))
                                accuracy = accuracy_score(ground_truth_values, predictions)
                                print(f"\tDokładność (zbiór testowy): {accuracy}\n")
                        </code></pre>
                        <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    Dokładność (zbiór testowy): 0.8064516129032258
                        </pre>
                    </div>
                    <h2>Dostrajanie parametrów</h2>
                    <p>Stosujemy algorytm zmniejszenia współczynnika uczenia się oraz dystansu. Prędkość zależy
                        od liczby iteracji. Zatem zakodowane są te dwa hiperparametry w tej liczbie, która zmienia nam
                        wyniki walidacji i pozwala wykryć, kiedy należy się zatrzymać, by uniknąć nadmiernego dopasowania.
                    </p> 
                    <div class="plot plot-small-width"><img src="img/som_hd/7.png"></img></div>
                    <p>Możemy też specjalnie użyć za dużej siatki. Wtedy zobaczymy dużo neuronów, które wciąż mają
                        losowe wagi, a więc uznawane są za niezainicjalizowane. Widać również, że z punktu widzenia
                        kwantyzacji macierz odległości próbuje tworzyć za dużo klastrów.
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/8.png"></img></div>
                        <div><img src="img/som_hd/9.png"></img></div>
                        <div><img src="img/som_hd/10.png"></img></div>
                    </div>
                </div>
        </div>
    </div>

    <footer>
        <div class="wrapper">
            <p>&copy; 2023 Piotr Dziedzic, Hubert Musiał, Hubert Pamuła, Tair Yerniyazov</p>
        </div>
    </footer>

    <script src="script/script.js"></script>
    <script src="script/prism.js"></script>
</body>

</html>