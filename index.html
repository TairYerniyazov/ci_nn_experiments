<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    <title>NN Experiments</title>
    <link rel="icon" href="img/favicon.ico">
    <link rel="stylesheet" href="style/styles.css">
    <link rel="stylesheet" href="style/prism.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nova+Square">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
</head>

<body>
    <header>
        <div class="wrapper">
            <div class="main_header">
                <div id="main_logo"><img src="img/uj_logo_white.png"></img></div>
                <div>
                    <h1>Inteligencja obliczeniowa</h1>
                    <p>Grupa 1.</p>
                </div>
            </div>
            <div class="menu">
                <p class="menu-item-on menu-item">Zadania</p>
                <p class="menu-item-off menu-item">Piotr Dziedzic</p>
                <p class="menu-item-off menu-item">Hubert Musiał</p>
                <p class="menu-item-off menu-item">Hubert Pamuła</p>
                <p class="menu-item-off menu-item">Tair Yerniyazov</p>
                <span class="material-symbols-outlined" id="font-changer">custom_typography</span>
            </div>
        </div>
    </header>
    <div class="content">
        <!-- GŁÓWNA STRONA -->
        <div class="wrapper page" id="main-page">
            <h2>Zadania</h2>
            <p>Dla obu wybranych zbiorów danych należy zaproponować sieć Feedforward oraz sieć SOM.
                Zadanie polega na przeprowadzeniu, opisaniu i podsumowaniu 4 eksperymentów.
            </p>
            <ol>
                <li><b>Opis zbiorów danych:</b> należy scharakteryzować zbiory danych:
                    <ul>
                        <li>Palmer Penguins;</li>
                        <li>Heart Disease;</li>
                    </ul>
                <li><b>Przygotowanie danych:</b> 
                    <ul>
                        <li>Opis wstępnej obróbki danych i sposobu podania ich sieci/kodowania - wpływ na liczbę neuronów.</li>
                        <li>Podział na zbiory: treningowy, testowy i walidujący (jak i dlaczego tak). 
                            Być może konieczne jest uwzględnienie rodzaju
                            sieci. Jeśli tak to dlaczego, jeśli nie to dlaczego?</li>
                    </ul>
                </li>
                <li><b>Struktura sieci:</b> przedstawić wybrane na początku konfiguracje obu sieci (dla obu problemów). 
                    Wszystkie decyzje uzasadnić.</li>
                <li><b>Uczenie sieci:</b> wskazać wybrane algorytmy i parametry uczenia sieci. Przedstawić osiągane rezultaty. 
                    Decyzje uzasadnić.</li>
                <li><b>Wyniki</b>: ocenić osiągnięte wyniki tj. dokładność i precyzję, złożoność i zbieżność (wykład). Tam, gdzie się da
                przedstawić i zinterpretować krzywą ROC.</li>
                <li><b>Dostrajanie parametrów:</b> krytyczna część prac, czyli co, czemu i jak było zmieniane, jakie dawało to efekty – dlaczego?</li>
                <li><b>Podsumowanie:</b> na zakończenie należy podsumować otrzymane wyniki dla 4 sieci. Zestawić je parami: raz dla danych, raz dla
                rodzaju sieci i wyciągnąć wnioski z otrzymanych wyników.</li>
            </ol>
        </div>

        <!-- STRONA PIOTRA D. -->
        <div class="wrapper page">
            <div class="plot plot-tiny-width"><img src="img/penguins_logo.png"></img></div>
            <h2>Opis zbioru danych</h2>
            <p>Zbiór został stworzony i udostępniony przez dr Kristen Gorman i stację Palmer Station na Antarktydzie,
                członka Długoterminowej Sieci Badań Ekologicznych wspieranego przez granty za pośrednictwem
                National Science Foundation, Office of Polar Programs (NSF-OOP). Dane zebrano w ramach badań mających
                na celu zbadanie zachowań żerowych pingwinów antarktycznych i ich związku ze zmiennością środowiska</p>
            <p>Dane pierwotnie zamieszczono w 3 oddzielnych zbiorach. Każdy odpowiadał jednemu z trzech gatunków
                Adelie (152 pingwiny), Gentoo (124 pingwiny) i Chinstrap 68 pingwinów. Połączono je w jeden zbiór danych
                o pingiwnach Palmera (344 pingwiny)
            </p>
            <div class="plot plot-small-height"><img src="img/ff_penguins/penguins_count.png"></img></div>
            <p>Mamy zatem 344 instacncje zawierające 8 atrybutów numerycznych i kategorycznych. Przedstawmy teraz krótko
                każdy z nich, w celu lepszego zrozumienia zagadnienia</p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
            <ul>
                <li><u>bill_length_mm</u> - Długość dzioba</li>
                <li><u>bill_depth_mm</u> - Wysokość dzioba</li>
                <li><u>flipper_length_mm</u> - Długość płetwy</li>
                <li><u>body_mass_g</u> - Masa Ciała pingwina</li>
                <li><u>year</u> - Rok w którym pingwin został zbadany</li>
            </ul>
        
            <b>Kategoryczne biarne</b>:
            <ul>
                <li><u>sex</u> - Płeć pingwina</li>
            </ul>
        
            <b>Kategoryczne</b>:
            <ul>
                <li><u>Species</u> - Gatunek pingwina</li>
                <li><u>Island</u> - Wyspa którą pingwin zamieszkiwał</li>
            </ul>
            </p>
            <p>Niestety 2 atrubuty "Year" oraz "Island" są dla nas bezużytczne w problemie klasyfikacji więc nie bierzemy
                ich pod uwagę w dalszych rozważaniach. Zatem liczba naszych "features" tak naprawdę zredukowała się do 5</p>
            <div class="plot plot-tiny-width"><img src="img/ff_penguins/bill_data_example.png"></img></div>
            <p>Zbadaliśmy rozkład 3 najistotniejszych cech czyli Bill Length, Bill Depth,
                Flipper Length i otrzymaliśmy następujące wyniki</p>
            <div class="plot"><img src="img/ff_penguins/bill_depth_distribution.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/bill_length_distribution.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/filpper_length_distribution.png"></img></div>
            <p>Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
                uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
                przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
                posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
                (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
                odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.</p>
            <div class="plot"><img src="img/ff_penguins/heatmapa.png"></img></div>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="#">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Zaczniemy od przetworzenie atrybutów
                kategorycznych. Mamy tutaj atrybuty, które
                są stricte kategoryczne (sex, species), tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
                konkretnych cech.</p>
            <p>Możemy te atrybuty (<code>sex</code> oraz <code>species</code>) rozbić na kilka kolumn.
                Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
                Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
                (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który za to odpowiada
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Zmiana kategorycznych wartosci na numeryczne
                    
                    # Kodowanie danych kategorycznych (one-hot encoding)
                    label_encoder = LabelEncoder()
                    
                    # One-hot encoding dla kolumny 'species'
                    palmer_penguins = pd.get_dummies(palmer_penguins, columns=['species'], prefix=['species'])
                    
                    # One-hot encoding dla kolumny 'sex'
                    palmer_penguins = pd.get_dummies(palmer_penguins, columns=['sex'], prefix='sex')
                    
                    print(palmer_penguins)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
        bill_length_mm bill_depth_mm    flipper_length_mm   body_mass_g \
    168         42.0            13.5             210.0       4150.0
    37          42.2            18.5             180.0       3550.0
    269         48.8            16.2             222.0       6000.0
    156         47.6            14.5             215.0       5400.0
    226         46.4            15.0             216.0       4700.0
    ..          ...             ...              ...         ...
    89          38.9            18.8             190.0       3600.0
    277         50.0            19.5             196.0       3900.0
    182         47.3            15.3             222.0       5250.0
    338         45.7            17.0             195.0       3650.0
    221         50.7            15.0             223.0       5550.0
    
        sex_female  sex_male
    168      1.0     0.0
    37       1.0     0.0
    269      0.0     1.0
    156      0.0     1.0
    226      1.0     0.0
    ..       ...     ...
    89       1.0     0.0
    277      0.0     1.0
    182      0.0     1.0
    338      1.0     0.0
    221      0.0     1.0

    [247 rows x 6 columns]
                </pre>
            </div>
            <p>Teraz powinniśmy się zastanowić jak postąpić z brakującymi wartościami. Możemy nieznane nam atrybuty
                douzupełnić wartością średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                344 instancje. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers">
                    <code class="language-python">
                        # Sprawdzenie, ile jest brakujących danych w każdej kolumnie
                        brakujace_dane = palmer_penguins.isnull().sum()
            
                        # Wyświetlenie liczby brakujących danych w każdej kolumnie
                        print(brakujace_dane)
                    </code>
                </pre>
                <code>
                    <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    bill_length_mm          2
    bill_depth_mm           2
    flipper_length_mm       2
    body_mass_g             2
    species_Adelie          0
    species_Chinstrap       0
    species_Gentoo          0
    sex_female              0
    sex_male                0
    dtype: int64
                    </pre>
                </code>
                <pre class="line-numbers">
                    <code class="language-python">
                        # Uzupełnianie wartości nieokreślonych średnią
            
                        # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                        srednia_bill_length_mm = palmer_penguins['bill_length_mm'].mean()
            
                        # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                        palmer_penguins['bill_length_mm'].fillna(srednia_bill_length_mm, inplace=True)
            
                        # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                        srednia_bill_depth_mm = palmer_penguins['bill_depth_mm'].mean()
            
                        # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                        palmer_penguins['bill_depth_mm'].fillna(srednia_bill_depth_mm, inplace=True)
            
                        # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                        srednia_flipper_length_mm   = palmer_penguins['flipper_length_mm'].mean()
            
                        # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                        palmer_penguins['flipper_length_mm'].fillna(srednia_flipper_length_mm, inplace=True)
            
                        # Oblicz średnią wartość kolumny 'bill_length_mm' (ignorując wartości null)
                        srednia_body_mass_g = palmer_penguins['body_mass_g'].mean()
            
                        # Uzupełnij brakujące wartości w kolumnie 'bill_length_mm' średnią wartością
                        palmer_penguins['body_mass_g'].fillna(srednia_body_mass_g, inplace=True)
                    </code>
                </pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w
                    tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych
            </p>
            <div class="code-snippet">
                <pre class="line-numbers">
                    <code class="language-python">
                        X = palmer_penguins.drop(["species_Adelie", "species_Chinstrap", "species_Gentoo"], axis=1)  # Wektory cech
                        y = palmer_penguins[["species_Adelie", "species_Chinstrap", "species_Gentoo"]]  # Wektory klas
                        
                        # Podział danych na zbiór treningowy (90%) i testowy (10%)
                        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
                        
                        # Kolejny podział zbioru X_temp na zbiór walidacyjny (20%) i treningowy (80%)
                        X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)
                        
                        scaler = StandardScaler()
                        
                        # Standaryzacja zbioru treningowego
                        X_train_scaled = scaler.fit_transform(X_train)
                        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = X_train.columns)
                        
                        # Standaryzacja zbioru walidacyjnego
                        X_valid_scaled = scaler.transform(X_valid)
                        X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns = X_valid.columns)
                        
                        # Standaryzacja zbioru testowego
                        X_test_scaled = scaler.transform(X_test)
                        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns = X_test.columns)
                    </code>
                </pre>
            </div>
            <p>Teraz wszystkie atrybuty mają wartości przedstawione numerycznie,
                a te, które określają kategorie, mają albo porządek, albo są zakodowane binarnie.
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci FeedForward</h2>
            <p>Sieć FeedForward (inaczej nazywana sztucznym neuronem, jednokierunkową siecią neuronową lub siecią
                jednokierunkową)
                to jedna z podstawowych architektur sztucznych sieci neuronowych. Jest to rodzaj sieci neuronowej, w której
                informacja
                przemieszcza się tylko w jednym kierunku, od warstwy wejściowej do warstw ukrytych i na koniec do warstwy
                wyjściowej,
                bez żadnych cykli czy sprzężeń zwrotnych.
            </p>
        
            <div class="plot plot-tiny-width"><img src="img/ff_penguins/siec_feed_forward.webp"></img></div>
            <div class="code-snippet">
                <pre class="line-numbers">
                    <code class="language-python">
                        # Sieć Feed Forward
                        model = Sequential()
    
                        # Dodanie warstw ukrytych
                        model.add(Dense(10, input_dim=X_train_scaled.shape[1], activation='relu'))
    
                        # Dodanie warstwy wyjściowej - zakładamy trzy klasy (species_Adelie, species_Chinstrap, species_Gentoo)
                        model.add(Dense(3, activation='softmax'))
                    </code>
                </pre>
            </div>
            <p>Struktura sieci:
            <p><code>Warstwa wejściowa (Input Layer):</code></p>
            <p>Zdefiniowana jako input_dim=X_train_scaled.shape[1],
                gdzie X_train_scaled.shape[1] to liczba cech w danych wejściowych po standaryzacji.
                To oznacza, że każdy neuron w tej warstwie odpowiada jednej zmiennej wejściowej w danych.
                Funkcja aktywacji: Brak funkcji aktywacji. Warstwa wejściowa jest liniową reprezentacją danych wejściowych.
            </p>
            <p><code>Pierwsza warstwa ukryta (Hidden Layer 1):</code></p>
            <p>
                Liczba neuronów: 10. Jest to arbitralnie wybrana liczba neuronów, które mają uczyć się nieliniowych zależności w
                danych.
                Funkcja aktywacji: ReLU (Rectified Linear Unit). ReLU jest często używana w warstwach ukrytych i ma postać f(x)
                = max(0, x),
                co oznacza, że jest nieliniowa i pomaga modelowi uczyć się nieliniowych wzorców.
            </p>
            <p><code>Warstwa wyjściowa (Output Layer):</code></p>
            <p>
                Liczba neuronów: 3. Liczba neuronów w warstwie wyjściowej odpowiada liczbie klas lub etykiet, które model ma
                przewidywać (species_Adelie, species_Chinstrap, species_Gentoo).
                Funkcja aktywacji: Softmax. Funkcja softmax jest często używana w warstwie wyjściowej do rozwiązywania problemów
                klasyfikacji wieloklasowej.
                Przekształca wyniki na prawdopodobieństwa przynależności do każdej z klas.
            </p>
            </p>
            <p>Wizualizacja modelu z pomocą biblioteki keras</p>
            <div class="code-snippet">
                <pre class="line-numbers">
                    <code class="language-python">
                        model.summary()
                        keras.utils.plot_model(model, to_file='test.png', show_shapes=True)
                    </code>
                </pre>
                <pre class="command-line">
    Model: "sequential"
    _________________________________________________________________
    Layer (type) Output Shape Param #
    =================================================================
    dense (Dense) (None, 10) 70
    
    dense_1 (Dense) (None, 3) 33
    
    =================================================================
    Total params: 103 (412.00 Byte)
    Trainable params: 103 (412.00 Byte)
    Non-trainable params: 0 (0.00 Byte)
                </pre>
            </div> 
            <div class="plot plot-small-height"><img src="img/ff_penguins/keras_visualization.png"></img></div>
            <h2>Uczenie sieci</h2>
            <p>Zanim przystąpimy do procesu uczenia się sieci, chcemy najpierw model "skompilować" tzn. skonfigurować go w
                odpowiedni sposób aby był już gotowy do procesu uczenia się. Określimy jak model ma być trenowany i
                jakie metryki zostaną obliczane podczas treningu</p>
            <p><code>loss</code></p>
            <p>Określa funkcję kosztu (ang. loss function), która jest używana do oceny różnicy
                między przewidywaniami modelu a rzeczywistymi etykietami
                (lub wartościami docelowymi). W tym przypadku używamy 'categorical_crossentropy'.
                Funkcja ta jest często stosowana w tego typu problemach.
            </p>
            <p><code>kilka słów o categorical crossentropy</code></p>
            <p>
                Categorical Cross-Entropy (czasami nazywana po prostu Cross-Entropy lub Log-Loss) to funkcja kosztu,
                która jest szeroko stosowana w problemach klasyfikacji wieloklasowej w uczeniu maszynowym i głębokim uczeniu.
                Jest to miara oceny różnicy między przewidywaniami modelu a rzeczywistymi etykietami (lub wartościami
                docelowymi)
                w przypadku, gdy zadanie polega na przyporządkowaniu obserwacji do jednej z wielu klas.
            </p>
            <p>1. Zakres i interpretacja: Funkcja kosztu Categorical Cross-Entropy jest używana w problemach klasyfikacji,
                gdzie dane należy przyporządkować do jednej z wielu klas (np. rozpoznawanie obrazów różnych gatunków zwierząt).
                Zakres wyników tej funkcji to liczby nieujemne, a jej wartość minimalna wynosi 0. Im niższa wartość funkcji
                kosztu,
                tym lepiej model dokonuje klasyfikacji.
            </p>
            <p>2. Rozkład prawdopodobieństwa: Przedstawmy sobie, że mamy C klas, a model generuje rozkład prawdopodobieństwa
                przynależności
                obserwacji do tych klas. Oznaczmy ten rozkład jako p(y|x), gdzie y to wektor prawdopodobieństw przynależności do
                każdej z klas.
                Model trenowany jest tak, aby ten rozkład prawdopodobieństwa był jak najbliższy rzeczywistemu rozkładowi etykiet
                (ground truth),
                który możemy oznaczyć jako q(y).</p>
            <p>3. Obliczenie kosztu: Koszt Categorical Cross-Entropy mierzy różnicę między rozkładem prawdopodobieństwa
                przewidywanym przez model (p(y|x))
                a rzeczywistym rozkładem (q(y)). Wzór na obliczenie kosztu dla pojedynczej próbki wygląda następująco:
        
                Loss = -Σ(q(y) * log(p(y|x)))
            </p>
            <p>W praktyce oblicza się tę sumę dla wszystkich klas (od 1 do C) i średnią po wszystkich próbkach treningowych.
                Ten wzór jest zapisem krzyżowej entropii (cross-entropy) między dwoma rozkładami prawdopodobieństwa.
                Warto zaznaczyć, że wartości logarytmów są zawsze ujemne, więc wartość funkcji kosztu jest nieujemna.</p>
            <p>
                4. Cel optymalizacji: Celem procesu treningu jest minimalizacja wartości Categorical Cross-Entropy.
                Oznacza to, że model jest uczony w taki sposób, aby jego przewidywania (rozkład p(y|x))
                były jak najbliższe rzeczywistym etykietom (q(y)).
            </p>
            <p><code>optimizer</code></p>
            <p>Określa algorytm optymalizacji, który będzie używany do aktualizacji wag modelu podczas treningu.
                W przykładzie 'adam' wskazuje na algorytm optymalizacji Adam.
                Adam jest popularnym algorytmem optymalizacji stosowanym w uczeniu maszynowym,
                który adaptacyjnie dostosowuje tempo uczenia w trakcie treningu. </p>
            <p><code>kilka słów o algorytmie Adam</code></p>
            <p>
                Adam (Adaptive Moment Estimation) to zaawansowany algorytm optymalizacji gradientowej stosowany w uczeniu
                maszynowym i głębokim uczeniu.
                Jest popularny ze względu na swoją skuteczność w praktyce i zdolność do dostosowywania tempo uczenia w trakcie
                treningu.
            </p>
            <p>
                Adam łączy w sobie dwie innowacje w dziedzinie optymalizacji: średnią ruchomą pierwszego momentu (ang. first
                moment)
                i średnią ruchomą drugiego momentu (ang. second moment). Oto podstawowe kroki, które podejmuje algorytm Adam w
                trakcie treningu:
            </p>
            <p>1. Inicjalizacja zmiennych:
        
                Algorytm inicjuje dwie zmienne: średnią ruchomą pierwszego momentu (zwana też eksponentjalnie ważoną średnią
                ruchomą gradientu) i średnią ruchomą drugiego momentu.
                Te zmienne inicjalizowane są na początku treningu jako wektory zerowe.
            </p>
            <p>
                2. Obliczenie gradientu:
        
                W każdej iteracji treningu obliczane są gradienty funkcji kosztu względem parametrów modelu.
            </p>
        
            <p>
                3. Aktualizacja pierwszego momentu (momentum):
        
                Pierwsza zmienna (moment) to średnia ruchoma gradientu (pierwszego momentu) z poprzednich iteracji.
                Jest to wygładzona wersja gradientu, która pomaga modelowi uniknąć utknięcia w lokalnym minimum.
            </p>
        
            <p>
                4. Aktualizacja drugiego momentu (second moment):
        
                Druga zmienna (moment) to średnia ruchoma kwadratu gradientu (drugiego momentu) z poprzednich iteracji.
                Jest to wskaźnik wariancji gradientu, co pozwala na adaptacyjne dostosowywanie tempa uczenia.
            </p>
        
            <p>
                5. Korekta obu momentów:
        
                Ponieważ oba momenty są inicjalizowane jako wektory zerowe, mogą być obarczone obciążeniem w kierunkach, w
                których gradienty są rzadkie.
                W celu korekty obciążenia algorytm Adam wykorzystuje skalowanie korekcyjne (bias correction), aby kompensować tę
                inicjalizację.
            </p>
            <p>
                6. Aktualizacja wag modelu:
        
                Parametry modelu (wagi) są aktualizowane na podstawie obliczonych momentów i gradientów.
                Dzięki uwzględnieniu obu momentów, algorytm Adam dostosowuje tempo uczenia w zależności od złożoności problemu i
                gradientów.
            </p>
            </p>
            <p><code>metrics</code></p>
            <p>Określa metryki, które zostaną obliczane podczas treningu i oceny modelu. W przykładzie używamy ['accuracy'],
                co oznacza, że podczas treningu i oceny modelu będzie obliczana dokładność (ang. accuracy), czyli stosunek
                poprawnie
                sklasyfikowanych próbek do wszystkich próbek</p>
            <div class="code-snippet">
                <pre class="line-numbers">
                    <code class="language-python">
                        # Kompilacja modelu
                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
                    </code>
                </pre>
            </div>
                <p>Teraz model jest gotowy do trenowania z użyciem danych treningowych i określonych kryteriów oceny.
                    Podczas procesu uczenia funkcja kosztu jest minimalizowana, a wag modelu są aktualizowane zgodnie z wybranym
                    algorytmem optymalizacji.
                    Metryki są obliczane, aby monitorować postęp treningu i ocenić wydajność modelu na danych walidacyjnych lub
                    testowych.</p>
            <div class="code-snippet">    
                <pre class="line-numbers">
                <code class="language-python">
                    # Trenowanie modelu
                    history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_valid, y_valid))
                </code>
            </pre>
            </div>
            <p><code>Parametry</code></p>
            <p>
                1. epochs: Parametr epochs określa liczbę epok treningu.
                Jedna epoka oznacza jedno przejście przez cały zbiór treningowy.
                W tym przypadku ustawienie epochs=10 oznacza, że model będzie trenowany przez 10 epok.
            </p>
            <p>
                2. batch_size: Parametr batch_size określa rozmiar partii (batch size) danych używanych w jednej iteracji
                treningu.
                Oznacza to, że podczas każdej epoki dane treningowe zostaną podzielone na partie o rozmiarze 10 próbek.
                Dla każdej partii model obliczy gradienty i zaktualizuje wagi
            </p>
            <p><code>Osiągniete rezultaty</code></p>
            <div class="plot"><img src="img/ff_penguins/rezultaty_epoki_text.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/rezultaty_epoki_wykresy.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/confussion_matrix.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/roc_curve.png"></img></div>
            <h2>Dostrajanie parametrów</h2>
            <p>W pierwotnym modelu zostało użyte tylko 10 epok, co jest stosunkowo małą liczbą,
                a dobranie większej liczby może konkretnie zwiększyć nasze rezultaty. Zatem sprawdźmy to</p>
            <div class="plot"><img src="img/ff_penguins/100_epok.png"></img></div>
            <p>Widzimy, że osiągneliśmy 98% dokładności na zbiorze testowym, używając 100 epok.
                Po przebiegu wykresu widzimy natomiast, że liczba ~45 epoo daje zadowalające rezultaty
            </p>
            <p>Spróbujmy teraz poeksperymentować z parametrem "batch-size" czyli
                rozmiaru parti używanej w jednej iteracji treningu. W pierwotnym modelu dobraliśmy jego wartość jako 10.
                Poniżej prezentuję rezultaty odpowiedno dla batch size równego 16, 32, 48
            </p>
            <div class="plot"><img src="img/ff_penguins/batch_size_16.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/batch_size_32.png"></img></div>
            <div class="plot"><img src="img/ff_penguins/batch_size_64.png"></img></div>
            <p>Bardzo łatwo zauważyć na wykresach, że im mniejszy batch size tym szybsza jest zbieżność</p>
            <p>Następnie przeprowadźmy eksperymenty z innymi algorytmami optymalizacji niż Adam</p>
            <p><code>Optimizer: SGD</code></p>
            <div class="plot"><img src="img/ff_penguins/sgd_optimizer.png"></img></div>
            <p><code>Rezultat: ()</code></p>
            <p><code>Optimizer: RMSProp</code></p>
            <div class="plot"><img src="img/ff_penguins/rmsprop_optimizer.png"></img></div>
            <p><code>Rezultat: ()</code></p>
            <p><code>Optimizer: Adagrad</code></p>
            <div class="plot"><img src="img/ff_penguins/adagrad.png"></img></div>
            <p><code>Rezultat: ()</code></p>
            </div>
        </div>


        <!-- STRONA HUBERTA M. -->
        <div class="wrapper page">
            <h3>SOM - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, 
            Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot plot-small-height"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA P. -->
        <div class="wrapper page">
            <h3>FFNN - Heart Disease</h3>
            <p>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.
                In particular, the Cleveland database is the only one that has been used by ML researchers to date.
                The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0
                (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to
                distinguish presence (values 1,2,3,4) from absence (value 0).
        </div>

        <!-- STRONA TAIRA Y. -->
        <div class="wrapper page">
            <h2>Opis zbioru danych</h2>
            <p>Zbiór danych <b><a href="https://archive.ics.uci.edu/dataset/45/heart+disease">
            "Heart Disease (Cleveland Clinic)"</a></b> dotyczy pacjentów, część z których ma chorobę serca.
            Mamy do dyspozycji 303 instancje zawierające 14 atrybutów numerycznych i kategorialnych.
            W trakcie dokonywania krótkiej analizy (<a href='https://colab.research.google.com/drive/1dRjVmMjxnA88s9SXnwcu2DvbYx6zUVKu?usp=sharing'>notatnik Jupyter, Google Colab)</a> podczas pracy naszego zespołu nad pierwszą częścią
            projektu zauważyliśmy dosyć nierównomierny rozkład poszczególnych atrybutów. Na szczęście
            dla wielu kolumn mamy rozkłady zbliżone do normalnych, więc po standaryzacji danych cały zbiór
            będzie wyglądał sensownie z punktu widzenia trenowania sieci. 
            <div class="row-with-images">
                <div><img src="img/som_hd/dataset_distribution_1.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_2.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_3.png"></img></div>
            </div>
            Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
            uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
            przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
            posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
            (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
            odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Pobranie repozytorium
                    !pip install ucimlrepo

                    # Zapis danych w postaci pandas dataframes
                    X = heart_diseases.data.features
                    y = heart_diseases.data.targets

                    # Wyliczenie macierzy korelacji
                    corr_matrix = heart_diseases.corr()
                    corr_matrix["num"].sort_values(ascending=False)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    num         1.000000
    oldpeak     0.504092
    cp          0.407075
    exang       0.397057
    slope       0.377957
    sex         0.224469
    age         0.222853
    restecg     0.183696
    trestbps    0.157754
    chol        0.070909
    fbs         0.059186
    thalach     -0.415040
    Name: num, dtype: float64
                </pre>
            </div>
            <p>Złą wiadomością jest to, że zbiór, który i tak zawiera za dużo atrybutów i za małą
                liczbę instancji, posiada rekordy cechujące się obecnością brakujących wartości
                w poszczególnych kolumnach (<code>ca</code> oraz <code>thal</code>). Mamy 2 rekordy z
                z wartościami <code>NaN</code> (Not a Number) w kolumnie <code>ca</code> oraz 4 rekordy, które
                są wypełniony tymi samymi pseudo-wartościami w kolumnie <code>thal</code>.
            </p>
            <p>Dobrą wiadomością jest stosunek liczby osób chorych do zdrowych. Mamy
                prawie że idealny rozbicie na <b>2 połówki (54% zdrowych i 46% chorych)</b>, więc
                nie musimy zastanawiać się nad metodami redukcji uprzedzeń występujących w zbiorze
                danych treningowych (idealnie oczywiście jest mieć w pełnie równomierny rozkład
                wartości przewidywanych w klasyfikacji binarnej, gdyż łatwiej będzie określić
                najniższy próg dokładności).
            </p>
            <p>Rozważmy teraz opis każdego z atrybutów danych wejściowych, które zostały dołączone
                do plików źródłowych autorów tego zbioru, gdyż w kolejnym etapie nam to się przyda:
            </p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
                <ul>
                    <li><u>age</u> - wiek;</li>
                    <li><u>trestbps</u> - spoczynkowe ciśnienie krwi;</li>
                    <li><u>chol</u> - cholesterol w surowicy</li>
                    <li><u>thalach</u> - osiągnięte maksymalne tętno;</li>
                    <li><u>oldpeak</u>- obniżenie odcinka ST wywołane wysiłkiem fizycznym w 
                        stosunku do odpoczynku;</li>
                    <li><u>cp</u> - liczba głównych naczyń (0-3) pokolorowanych metodą 
                        fluorosopii.</li>
                </ul>

                <b>Kategorialne biarne</b>:
                <ul>
                    <li><u>sex</u> - płeć;</li>
                    <li><u>fbs</u> - cukier we krwi na czczo;</li>
                    <li><u>exang</u> - dławica piersiowa wywołana wysiłkiem fizycznym.</li>
                </ul>

                <b>Kategorialne uporządkowane</b>:
                <ul>
                    <li><u>restecq</u> - spoczynkowe wyniki elektrokardiograficzne 
                        (0 - norma, 1 - anomalia, 2 - hipertrofia);</li>
                    <li><u>slope</u> - nachylenie szczytowego odcinka ST podczas wysiłku 
                        (0 - wznoszące się, 2 - płaskie, 3 - opadający).</li>
                </ul>
    
                <b>Stricte kategorialne</b>:
                <ul>
                    <li><u>cp</u> - rodzaj bólu w klatce piersiowej (cztery typy);</li>
                    <li><u>thal</u> - thal = niedokrwistość tarczowatokrwinkowa (trzy typy).</li>
                </ul>
            </p>
            <p>Oczywiscie, niektóre atrybute są bardziej istotne, inne mniej. Teraz
                nie możemy stwierdzać nic poza określeniem współczynnika korelacji.
                Jeśli połączymy razem wartości opisujące wiek oraz puls pacjenta, uzyskamy nietrywialny wykres,
                na którym dobrze widać medianę próbek względem stosunku tych liczb:
            </p>
            <div class="plot"><img src="img/som_hd/correlation_1.png"></img></div>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="#">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Przede wszystkim powinniśmy jakoś postąpić z brakującymi wartościami. Ze zwględu na to,
                że każdy rekord zawiera 13 cech, to możemy te dwie znane nam kolumny douzupełnić wartością
                średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                303 instancje, ani nie stać nas na wyrzucenie całych tych kolumn, gdyż już na etapie
                sprawdzenie zależności liniowych (korelacji) z atrybutem przewidywanym wykazały się
                dosyć poważnie. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Rzutowanie wszystkich kolumn na liczby zmiennoprzecinkowe
                    X = X.astype("float")

                    # Imputacja wartości
                    from sklearn.impute import SimpleImputer
                    import numpy as np
                    
                    imputer = SimpleImputer(missing_values = np.NaN, strategy="median")
                    imputer.fit(X[['ca', 'thal']])
                    imputed_X_columns = imputer.transform(X[['ca', 'thal']])
                    
                    X.loc[:, ('ca', 'thal')] = imputed_X_columns
                </code></pre>
            </div>
            <p>Kolejna rzecz, którą musimy załatwić, polega na przerzucenia wartości przewidywanego
                atrybutu na liczby 0 i 1, gdyż wszystko powyżej 0 wskazuje na obecność choroby i nie 
                dodaje żadnego innego znaczenia. Zróbmy to teraz:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Zamiana przewidywanego atrybutu na wartości binarne zgodnie z opisem zbioru
                    y.loc[y['num'] > 0, 'num'] = 1.0
                </code></pre>
            </div>
            </p>Następnie na naszej liście zadań do zrobienia mamy oczwywiście przetworzenie atrybutów
            kategorialnych. Tak jak wspomnieliśmy wcześniej, niektóre z nich są binarne, więc z nimi
            nie będzie żadnego problemu, gdyż mieszczą się w przedziale od 0 do 1. Inne mają porządek
            semantyczny i określają stopień konkretnej cechy, więc też je możemy bezpiecznie zostawić, a
            później standaryzować jako atrybuty numeryczne. Zostaje nam problem tych atrybutów, które
            są stricte kategorialne, tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
            konkretnych cech.
            <p>Możemy te atrybutu (<code>cp</code> oraz <code>thal</code>) rozbić na kilka kolumn.
            Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
            Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
            (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który to robi
            dla atrybutu <code>cp</code>:
            </p> 
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X_cp = X[['cp']]
                    
                    # Zakodujmy najpierw "gorącojedynkowo" wszystkie cztery typy 'cp'
                    from sklearn.preprocessing import OneHotEncoder
                    
                    cat_encoder = OneHotEncoder()
                    X_cp_1hot = cat_encoder.fit_transform(X_cp)

                    # Teraz mamy dodatkowe 4 kolumny (każda odpowiada za obecność 
                    # określonego rodzaju bólu w klatce piersiowej):
                    print(X_cp_1hot.toarray())
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    [[1. 0. 0. 0.]
    [0. 0. 0. 1.]
    [0. 0. 0. 1.]
    ...
    [0. 0. 0. 1.]
    [0. 1. 0. 0.]
    [0. 0. 1. 0.]]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Pierwotne etykiety (numery typu bólu, 1-4) mamy wciąż pod ręką
                    cat_encoder.categories_
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    [array([1., 2., 3., 4.])]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Dodajmy teraz te typy zakodowane binarnie do naszych danych i wyrzućmy kolumnę 'cp'
                    for i in range(4):
                        X[f"cp_type_{i + 1}"] = X_cp_1hot.toarray()[:, i].reshape(-1, 1)
                    X.drop(columns=["cp"], inplace=True)
                </code></pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w 
                tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych (oprócz atrybutów binarnych):
            <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                from sklearn.model_selection import train_test_split
        
                # Podział na zbióry: testowy, treningowy
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)
                
                # Standaryzacja
                from sklearn.preprocessing import StandardScaler

                num_columns = ['age', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca']
                cat_columns = ['sex', 'fbs', 'exang', 'cp_type_1', 'cp_type_2', 'cp_type_3', 'cp_type_4', 'thal_3', 'thal_6', 'thal_7']
                X_train_cat = X_train.loc[:, cat_columns]
                X_train_num = X_train.loc[:, num_columns]

                std_scaler = StandardScaler()
                X_train_num_scaled = std_scaler.fit_transform(X_train_num)
                X_train = np.concatenate((X_train_num_scaled, X_train_cat.to_numpy()), axis=1)
                
                y_train = y_train.to_numpy()
            </code></pre></div>
            <p>Jeśli chodzi o dane do walidacji, to sensownym rozwiązaniem wydaje się być
                stosowanie K-składową walidacji krzyżowej (z ang. <b>K-Fold Cross Validation</b>):
                <div class="plot"><img src="img/som_hd/k_fold_validation.png"></img></div>
                <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                    k_folds = 5
                    num_val_samples = len(X_train) // k_folds
                    num_last_val_samples = len(X_train) % k_folds
                    
                    X_validation_sets = []
                    y_validation_sets = []
                    
                    X_train_sets = []
                    y_train_sets = []
                    
                    for i in range(k_folds):
                        print('Processing fold #', i)
                        X_validation_sets.append(X_train[i * num_val_samples: (i + 1) * num_val_samples])
                        y_validation_sets.append(y_train[i * num_val_samples: (i + 1) * num_val_samples])
                        X_train_sets.append(np.concatenate([X_train[:i * num_val_samples],
                        X_train[(i + 1) * num_val_samples:]], 
                        axis=0))
                        y_train_sets.append(np.concatenate([y_train[:i * num_val_samples],
                        y_train[(i + 1) * num_val_samples:]],
                        axis=0))
                    
                        # Dodajemy resztę do ostaniego zbioru walidacyjnego
                        if i == (k_folds - 1):
                        X_validation_sets[k_folds - 1] = np.concatenate((X_validation_sets[k_folds - 1], X_train[- num_last_val_samples:]),
                        axis=0)
                        y_validation_sets[k_folds - 1] = np.concatenate((y_validation_sets[k_folds - 1], y_train[- num_last_val_samples:]),
                        axis=0)
                </code></pre></div>
            <p>Teraz bodajże wszystkie atrybuty mają wartości przedstawione numerycznie, 
                a te, które określają kategorie, mają albo porządek, albo są zakodowane binarnie. 
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci SOM</h2>
            <p>Wiemy, oczywiście, że mamy do czynienia z problemem klasyfikacji, więc moglibyśmy 
                od razu w przestrzeni neuronów ustawić tylko dwie jednostki (jedna odpowiadałaby
                wówczas za osób chorych, inny za zdrowych). Jednak nie będzie to
                prawdziwa SOM, tylko zwyczajne wykonanie algorytmu LVQ, stosując uczenie nadzorowane.
        
            <p>Spróbujmy zatem postąpić inaczej. Wyobraźmy sobie, że nie wiemy, że mamy dane naszych 
                pacjęntów i nie wiemy, czy wgl oni się wyodrębniają w jakieś osobne kategorie czy nie. 
                Wtedy mamy uczenie nienadzorowane, więc klastry "osoby chore" i "osoby zdrowe" 
                powstaną naturalnie (co najmniej w teorii, bo możliwe, że sięc nam zamiast tego np.
                rozdzieli ludzi na kobiet i mężczyzn, ale to jest mniej prawdopodobne, bo 17 kolumn 
                wskazują prawdopodobnie na to, że płeć nie stanowi za dużą różnicę w przypadku danych
                medycznych dotyczących chorób serca).

            <p><b>Architektura sieci:</b><p>
            <ul>
                <li><u>Siatka</u> - 2D, 9x9;</li>
                <li><u>Liczba neuronów</u> - 81 (zasada heurystyczna dla treningowego zbioru danych
                    o rozmiarze 272 rekordy);</li>
                <li><u>Wymiar wektorów wag</u> - 18 (co odpowiada liczbie atrybutów);</li>
                <li><u>Odległość</u> - metryka Euklidesowa</li>
                <li><u>Liczenie sąsiedztwa</u> - metryka Manhattan;</li>
                <li><u>Redukcja współczynników</u> - zależy od maksymalnych wartości oraz całkowitej
                liczby iteracji.</li>
            </ul>
            <div class="plot"><img src="img/som_hd/som_nn.png"></img></div>
            <p>Skoro mamy w zbiorze treningowym 272 atrybuty, to zgodnie z zasadą kciuka, która mówi, że
                liczba neuronów to 5 * sqrt(N), lepiej jest tworzyć siatkę 9x9, co właśnie zrobiliśmy. 
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Inicjalizacja parametrów siatki
                    n_dimensions = 18
                    grid_size = 9
                    
                    # Inicjalizacja wag neuronow (do klasteryzacji)
                    initial_weights = np.zeros((grid_size, grid_size, n_dimensions))
                    for i in range(grid_size):
                    for j in range(grid_size):
                    for k in range(n_dimensions):
                    initial_weights[i, j, k] = np.random.uniform(-1, 1, 1)
                </code></pre>
            </div>
            <p>Metryka Manhattan pozwala na szybkie i proste liczenie promienia obszaru
                sąsiedztwa, gdyż jest to zwykła norma L1 (Lasso).
            </p>
            <h2>Uczenie sieci</h2>
                <p>Algorytmów do wyboru w przypadku sieci SOM nie mamy za dużo. Z definicji sieci
                    jest to tzw. Online learning. Czyli dokładamy siatki neuronów o losowo zainincjalizowanych
                    wagach kolejne próbki i wyłuskujemy neurona-zwycięzcę (mierzymy to za pomocą metryki
                    Euklidesowe: (przypisane wagi neuronu, atrybuty/kolumny próbki)). Może jedyna rzecz,
                    na którą warto zwrócić uwagę to to, że obszar sąsiedztwa operuje nie w przestrzenie wag
                    neuronów (de facto przestrzeni danych wejściowych), tylko na samej siatce, która jest 
                    statyczna, czyli położenie neuronów się nigdy nie zmienia. Na wielu wizualizacjach dostępnych
                    w Internecie często pokazuje się, jak siatka ta rozciąga się i przyjmuje kształt danych wejściowych.
                    Oczywiście w taki sposób należy rozumieć zmianę/dostrajanie wag. Natomiast tak wizualnie, to jest
                    raczej na odwrót. To próbki są rozpinane na siatce rzucane w taki sposób, że każdy rekord z X znajdzie
                    swój neuron w Y. Później patrzymy na siatkę i widzimy (co najmniej mamy taką nadzieję) klastry.
                </p>
                <p><b>Wybrane parametry:</b>
                <p>
                <ul>
                    <li><u>Learning rate</u> - 0.9;</li>
                    <li><u>Maksymalny promień obszaru sąsiedztwa</u> - 10;</li>
                    <li><u>Liczba iteracji ("epoki" * liczba_próbek)</u> - 6528;</li>
                </ul>
                <div class="code-snippet">
                    <pre class="line-numbers"><code class="language-python">
                        def train_SOM(n_epochs, X_part, y_part, weights, n_dimensions, grid_size, show_plots=False,
                                    progress_bar=False, max_lr=.9, max_ds=10):
                            # Hyperparametry modelu
                            max_learning_rate = max_lr
                            learning_rate = max_learning_rate

                            max_distance = max_ds
                            dist = max_distance

                            epochs = n_epochs
                            n_samples = len(X_part)

                            total_n_steps = epochs * n_samples
                            step = 0

                            if show_plots:
                                plot_U_matrix(step, weights, grid_size)
                                plot_labels(step, grid_size, weights, X_part, y_part)
                                plot_labels(step, grid_size, weights, X_part, y_part, majority_voting=True)

                            # Trenowanie sieci SOM
                            for _ in range(epochs):
                                # Iterowanie po wszystkich próbkach z przestrzeni wejściowej
                                for sample_index in range(n_samples):

                                # Aktualizacja współczynnika uczenia się oraz dystansu funkcji sąsiedzstwa
                                learning_rate, dist = decay(step, total_n_steps, max_learning_rate, max_distance)
                                sample = X_part[sample_index]

                                # Liczenie odległości od wszystkich neuronów
                                distances = [[euclidian_distance(sample, weights[i, j]) for j in range(grid_size)] for i in range(grid_size)]

                                # Znalezienie neurona-zwycięzcy
                                winner_index = np.argmin(distances)

                                # Wyłuskanie indeksu zwycięzcy
                                row = winner_index // grid_size
                                col = winner_index % grid_size

                                # Aktualizacja wag wszystkich neuronów
                                for j in range(grid_size):
                                for i in range(grid_size):
                                if man([row, col], [i, j]) <= dist: weights[i, j]=weights[i, j] + learning_rate * (sample - weights[i, j]) 
                                
                                # Rysowanie wykresów 
                                if step % 1000==0: 
                                    if progress_bar: 
                                        sys.stdout.write(f"\rStep {step + 1}\n") 
                                    if show_plots: 
                                        if step % 1000 == 0: 
                                            plot_U_matrix(step + 1, weights, grid_size) 
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part)
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part, majority_voting=True) 
                               
                                # Zliczanie liczby iteracji
                                step=step + 1 
                        
                        # Zwrócenie macierzy, którą będziemy mogli wykorzystać do predykcji return np.copy(weights),
                        get_prediction_matrix(grid_size, weights, X_part, y_part, majority_voting=True)
                    </code></pre>
                </div>
                    <p>Oczywiście dostrajaliśmy parametry, o czym powiemy później. W trakcie trenowani odwołaliśmy się
                        do własnych zdefiniowanych funkcji, które rysowały nam odpowiednie wykresy. Najpierw
                        po każdej iteracji kolorowaliśmy stany neuronów (czerowny - trafił chory pacjent, niebieski - 
                        trafił zdrowy pacjent, szary - jeszcze nikt nie trafił i wagi neurona nie są zmieniane):
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/1.png"></img></div>
                        <div><img src="img/som_hd/2.png"></img></div>
                    </div>
                    <p>Możemy też zobaczyć jak wygląda macierz odległości (U-Matrix). Mapa odległości (U-Matrix) 
                        pokazuje nam odległość pomiędzy wagami sąsiednich neuronów, a to właśnie te wagi dostrajane
                    są do kształtu danych z przestrzeni wejściowej. Zatem im dalej dwa neurony od siebie się znajdują w sensie "wagowym",
                    tym bardziej różniący się od siebie próbki te neurony chwyciły. Na mapie podczas klasteryzacji tworzą się ściany, które
                    separują nam zgrupowane rekordy.</p> 
                    <div class="row-with-images">
                        <div><img src="img/som_hd/3.png"></img></div>
                        <div><img src="img/som_hd/4.png"></img></div>
                    </div>
                    <p>Żeby móc klasyfikować próbki ze zbioru walidacyjnego, użyliśmy tak zwanego Majority Voting, czyli głosowania.
                        Innymi słowy, jeśli do neuronu trafiają 4 czerwone punkty i 8 niebieskich, to uznajemy, że ten neuron nadaje
                        nowym próbkom etykietę "osoba zdrowa". Jeżeli mamy w głosowaniu niejednoznaczność albo neuron w ogóle
                        nie złapał żadnego punktu podczas trenowania, to oznaczamy na szaro taki neuron jako niezainicjalizowany.
                        Podczas klasyfikacji zachowujemy wtedy stabilność statystyczną i próbujemy zgadnąć (czyli prawdopodobieństwo = 50%).
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/5.png"></img></div>
                        <div><img src="img/som_hd/6.png"></img></div>
                    </div>
                    <h2>Wyniki</h2>
                    <p>Możemy zobaczyć końcowy wynik (po wcześniejszej walidacji, oczywiście, którą przedstawimy w kolejnej części):</p>
                    <div class="code-snippet">
                        <pre class="line-numbers"><code class="language-python">
                            from sklearn.metrics import accuracy_score

                            # Końcowa ewaluacja na zbiorze testowym
                            ground_truth_values = y_test
                            predictions = []
                            
                            for sample_index in range(len(X_test)):
                                predictions.append(predict_sample(X_test[sample_index], prediction_matrix, weights))
                                accuracy = accuracy_score(ground_truth_values, predictions)
                                print(f"\tDokładność (zbiór testowy): {accuracy}\n")
                        </code></pre>
                        <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
    Dokładność (zbiór testowy): 0.8064516129032258
                        </pre>
                    </div>
                    <h2>Dostrajanie parametrów</h2>
                    <p>Stosujemy algorytm zmniejszenia współczynnika uczenia się oraz dystansu. Prędkość zależy
                        od liczby iteracji. Zatem zakodowane te dwa hyperparametry w tę liczbę, która zmienia nam
                        wyniki walidacji i pozwala wykryć, kiedy należy się zatrzymać, by uniknąć nadmiernego dopasowania.
                    </p> 
                    <div class="plot plot-small-width"><img src="img/som_hd/7.png"></img></div>
                    <p>Możemy też specjalnie użyć za dużej siatki. Wtedy zobaczymy dużo neuronów, które wciąż mają
                        losowe wagi, a więc uznawane są za niezainicjalizowane. Widać również, że z punktu widzenia
                        kwantyzacji macierz odległości próbuje tworzyć za dużo klastrów.
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/8.png"></img></div>
                        <div><img src="img/som_hd/9.png"></img></div>
                        <div><img src="img/som_hd/10.png"></img></div>
                    </div>
                </div>
        </div>
    </div>

    <footer>
        <div class="wrapper">
            <p>&copy; 2023 Piotr Dziedzic, Hubert Musiał, Hubert Pamuła, Tair Yerniyazov</p>
        </div>
    </footer>

    <script src="script/script.js"></script>
    <script src="script/prism.js"></script>
</body>

</html>