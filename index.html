<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">
    <title>NN Experiments</title>
    <link rel="icon" href="img/favicon.ico">
    <link rel="stylesheet" href="style/styles.css">
    <link rel="stylesheet" href="style/prism.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nova+Square">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
</head>

<body>
    <header>
        <div class="wrapper">
            <div class="main_header">
                <div id="main_logo"><img src="img/uj_logo_white.png"></img></div>
                <div>
                    <h1>Inteligencja obliczeniowa</h1>
                    <p>Grupa 1.</p>
                </div>
            </div>
            <div class="menu">
                <p class="menu-item-on menu-item">Zadania</p>
                <p class="menu-item-off menu-item">Piotr Dziedzic</p>
                <p class="menu-item-off menu-item">Hubert Musiał</p>
                <p class="menu-item-off menu-item">Hubert Pamuła</p>
                <p class="menu-item-off menu-item">Tair Yerniyazov</p>
                <span class="material-symbols-outlined" id="font-changer">custom_typography</span>
            </div>
        </div>
    </header>
    <div class="content">
        <!-- GŁÓWNA STRONA -->
        <div class="wrapper page" id="main-page">
            <h2>Zadania</h2>
            <p>Dla obu wybranych zbiorów danych należy zaproponować sieć Feedforward oraz sieć SOM.
                Zadanie polega na przeprowadzeniu, opisaniu i podsumowaniu 4 eksperymentów.
            </p>
            <ol>
                <li><b>Opis zbiorów danych:</b> należy scharakteryzować zbiory danych:
                    <ul>
                        <li>Palmer Penguins;</li>
                        <li>Heart Disease;</li>
                    </ul>
                <li><b>Przygotowanie danych:</b> 
                    <ul>
                        <li>Opis wstępnej obróbki danych i sposobu podania ich sieci/kodowania - wpływ na liczbę neuronów.</li>
                        <li>Podział na zbiory: treningowy, testowy i walidujący (jak i dlaczego tak). 
                            Być może konieczne jest uwzględnienie rodzaju
                            sieci. Jeśli tak to dlaczego, jeśli nie to dlaczego?</li>
                    </ul>
                </li>
                <li><b>Struktura sieci:</b> przedstawić wybrane na początku konfiguracje obu sieci (dla obu problemów). 
                    Wszystkie decyzje uzasadnić.</li>
                <li><b>Uczenie sieci:</b> wskazać wybrane algorytmy i parametry uczenia sieci. Przedstawić osiągane rezultaty. 
                    Decyzje uzasadnić.</li>
                <li><b>Wyniki</b>: ocenić osiągnięte wyniki tj. dokładność i precyzję, złożoność i zbieżność (wykład). Tam, gdzie się da
                przedstawić i zinterpretować krzywą ROC.</li>
                <li><b>Dostrajanie parametrów:</b> krytyczna część prac, czyli co, czemu i jak było zmieniane, jakie dawało to efekty – dlaczego?</li>
                <li><b>Podsumowanie:</b> na zakończenie należy podsumować otrzymane wyniki dla 4 sieci. Zestawić je parami: raz dla danych, raz dla
                rodzaju sieci i wyciągnąć wnioski z otrzymanych wyników.</li>
            </ol>
        </div>

        <!-- STRONA PIOTRA D. -->
        <div class="wrapper page">
            <div class="plot plot-small-height"><img src="img/penguins_logo.png"></img></div>
            <h2>Opis zbioru danych</h2>
            <p>Zbiór został stworzony i udostępniony przez dr Kristen Gorman i stację Palmer Station na Antarktydzie,
                członka Długoterminowej Sieci Badań Ekologicznych wspieranego przez granty za pośrednictwem
                National Science Foundation, Office of Polar Programs (NSF-OOP). Dane zebrano w ramach badań mających
                na celu zbadanie zachowań żerowych pingwinów antarktycznych i ich związku ze zmiennością środowiska</p>
            <p>Dane pierwotnie zamieszczono w 3 oddzielnych zbiorach. Każdy odpowiadał jednemu z trzech gatunków
                Adelie (152 pingwiny), Gentoo (124 pingwiny) i Chinstrap 68 pingwinów. Połączono je w jeden zbiór danych
                o pingiwnach Palmera (344 pingwiny)
            </p>
            <div class="plot plot-small-height"><img src="img/ff_penguins/penguins_count.png"></img></div>
            <p>Mamy zatem 344 instacncje zawierające 8 atrybutów numerycznych i kategorycznych. Przedstawmy teraz krótko
                każdy z nich, w celu lepszego zrozumienia zagadnienia</p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
            <ul>
                <li><u>bill_length_mm</u> - Długość dzioba</li>
                <li><u>bill_depth_mm</u> - Wysokość dzioba</li>
                <li><u>flipper_length_mm</u> - Długość płetwy</li>
                <li><u>body_mass_g</u> - Masa Ciała pingwina</li>
                <li><u>year</u> - Rok w którym pingwin został zbadany</li>
            </ul>

            <b>Kategoryczne binarne</b>:
            <ul>
                <li><u>sex</u> - Płeć pingwina</li>
            </ul>

            <b>Kategoryczne</b>:
            <ul>
                <li><u>Species</u> - Gatunek pingwina</li>
                <li><u>Island</u> - Wyspa którą pingwin zamieszkiwał</li>
            </ul>
            </p>
            <p>Niestety 2 atrubuty "Year" oraz "Island" są dla nas bezużytczne w problemie klasyfikacji więc nie bierzemy
                ich pod uwagę w dalszych rozważaniach. Zatem liczba naszych "features" tak naprawdę zredukowała się do 5</p>
            <div class="plot plot-small-height"><img src="img/ff_penguins/bill_data_example.png"></img></div>
            <p>Zbadaliśmy rozkład 3 najistotniejszych cech czyli Bill Depth, Bill Length,
                Flipper Length i otrzymaliśmy następujące wyniki: mamy dobrą dystrybucję trzech istotnych atrybutów numerycznych, 
                która wygląda na rozkład normalny. Zatem, pod warunkiem,
                że dane ze zbioru testowego, a później produkcyjnego (przemilczmy, że klasyfikacja pingwinów nam się nie przyda w
                Krakowie....) będą się cechowały taką samą wariancją, to model dobrze sobie poradzi, gdyż spodziewamy się, że odchylenie
                standardowe będzie ograniczało liczbę tzw. "Outliers"'ów, czyli wartości odbiegających od mediany i średniej.</p>
            <div class="plot plot-small-width"><img src="img/ff_penguins/bill_depth_distribution.png"></img></div>
            <div class="plot plot-small-width"><img src="img/ff_penguins/bill_length_distribution.png"></img></div>
            <div class="plot plot-small-width"><img src="img/ff_penguins/flipper_length_distribution.png"></img></div>
            <p>Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
                uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
                przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
                posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
                (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
                odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.</p>
            <div class="plot"><img src="img/ff_penguins/heatmapa.png"></img></div>
            <p>w pierwszym rzędzie macierzy widać wzdłuż kolumn, że wartości liczbowe odgrywają kluczową rolę w predykcji atrybuty
            "species". Odwrotna obserwacja ma miejsce w przypadku płci (można przypuszczać, że female penguins nie zbyt mocno się
            różnią od male penguins). Dodakowo, patrząc na ostatnie dwa rzędy, widzimy że stosunek liczby female do liczby male jest
            idealny, więc mamy dobre dane po załatwieniu rekordów zawierajacych brakujące wartości. Dla przypomnienia: macierz
            korelacji jest zawsze symetryczna, gdyż odzwierciedla relacje liniowe, a więc można zawsze to odwrócić (prawie, ale nie ma to
            wpływu na korelację, gdyż gdy jedna z kolumn jest wartością stałą, to w dwóch miejscach się ustawia 0). 
            Wdzłuż przekątnej są same jedynki, gdyż każdy atrybut ma 100% korelacji z samym sobą z definicji.</p>
        </div>


        <!-- STRONA HUBERTA M. -->
        <div class="wrapper page">
            <h3>SOM - Palmer Penguin</h3>
            <p>Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, 
            Antarctica LTER, a member of the Long Term Ecological Research Network.</p>
            <div class="plot plot-small-height"><img src="img/penguins_logo.png"></img></div>
        </div>


        <!-- STRONA HUBERTA P. -->
        <div class="wrapper page">
            <h3>FFNN - Heart Disease</h3>
            <p>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.
                In particular, the Cleveland database is the only one that has been used by ML researchers to date.
                The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0
                (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to
                distinguish presence (values 1,2,3,4) from absence (value 0).
        </div>

        <!-- STRONA TAIRA Y. -->
        <div class="wrapper page">
            <h2>Opis zbioru danych</h2>
            <p>Zbiór danych <b><a href="https://archive.ics.uci.edu/dataset/45/heart+disease">
            "Heart Disease (Cleveland Clinic)"</a></b> dotyczy pacjentów, część z których ma chorobę serca.
            Mamy do dyspozycji 303 instancje zawierające 14 atrybutów numerycznych i kategorialnych.
            W trakcie dokonywania krótkiej analizy (<a href='https://colab.research.google.com/drive/1dRjVmMjxnA88s9SXnwcu2DvbYx6zUVKu?usp=sharing'>notatnik Jupyter, Google Colab)</a> podczas pracy naszego zespołu nad pierwszą częścią
            projektu zauważyliśmy dosyć nierównomierny rozkład poszczególnych atrybutów. Na szczęście
            dla wielu kolumn mamy rozkłady zbliżone do normalnych, więc po standaryzacji danych cały zbiór
            będzie wyglądał sensownie z punktu widzenia trenowania sieci. 
            <div class="row-with-images">
                <div><img src="img/som_hd/dataset_distribution_1.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_2.png"></img></div>
                <div><img src="img/som_hd/dataset_distribution_3.png"></img></div>
            </div>
            Korzystając ze standardowych narzędzi dostępnych w bibliotece Pandas możemy łatwo
            uzyskać pełną macierz korelacji cech z przewidywaną wartością. Powinniśmy pamiętać
            przede wszystkim, że taki przegląd pozwala zaledwie na zapoznanie się ze zbiorem i może
            posłużyć nam jako wsparcie heurystyczne podczas pracy nad tzw. inżynierią cech
            (z ang. <b>Feature Engineering</b>). Współczynnik korelacji nie gwarantuje nam poprawnego
            odzwierciedlenia relacji nieliniowych, które z pewnością występują w naszym złożonym zbiorze.
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Pobranie repozytorium
                    !pip install ucimlrepo

                    # Zapis danych w postaci pandas dataframes
                    X = heart_diseases.data.features
                    y = heart_diseases.data.targets

                    # Wyliczenie macierzy korelacji
                    corr_matrix = heart_diseases.corr()
                    corr_matrix["num"].sort_values(ascending=False)
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                num 1.000000
                oldpeak 0.504092
                cp 0.407075
                exang 0.397057
                slope 0.377957
                sex 0.224469
                age 0.222853
                restecg 0.183696
                trestbps 0.157754
                chol 0.070909
                fbs 0.059186
                thalach -0.415040
                Name: num, dtype: float64
                </pre>
            </div>
            <p>Złą wiadomością jest to, że zbiór, który i tak zawiera za dużo atrybutów i za małą
                liczbę instancji, posiada rekordy cechujące się obecnością brakujących wartości
                w poszczególnych kolumnach (<code>ca</code> oraz <code>thal</code>). Mamy 2 rekordy z
                z wartościami <code>NaN</code> (Not a Number) w kolumnie <code>ca</code> oraz 4 rekordy, które
                są wypełniony tymi samymi pseudo-wartościami w kolumnie <code>thal</code>.
            </p>
            <p>Dobrą wiadomością jest stosunek liczby osób chorych do zdrowych. Mamy
                prawie że idealny rozbicie na <b>2 połówki (54% zdrowych i 46% chorych)</b>, więc
                nie musimy zastanawiać się nad metodami redukcji uprzedzeń występujących w zbiorze
                danych treningowych (idealnie oczywiście jest mieć w pełnie równomierny rozkład
                wartości przewidywanych w klasyfikacji binarnej, gdyż łatwiej będzie określić
                najniższy próg dokładności).
            </p>
            <p>Rozważmy teraz opis każdego z atrybutów danych wejściowych, które zostały dołączone
                do plików źródłowych autorów tego zbioru, gdyż w kolejnym etapie nam to się przyda:
            </p>
            <p>
                <b>Numeryczne</b> (w tym te, które mają ograniczony zakres):
                <ul>
                    <li><u>age</u> - wiek;</li>
                    <li><u>trestbps</u> - spoczynkowe ciśnienie krwi;</li>
                    <li><u>chol</u> - cholesterol w surowicy</li>
                    <li><u>thalach</u> - osiągnięte maksymalne tętno;</li>
                    <li><u>oldpeak</u>- obniżenie odcinka ST wywołane wysiłkiem fizycznym w 
                        stosunku do odpoczynku;</li>
                    <li><u>cp</u> - liczba głównych naczyń (0-3) pokolorowanych metodą 
                        fluorosopii.</li>
                </ul>

                <b>Kategorialne biarne</b>:
                <ul>
                    <li><u>sex</u> - płeć;</li>
                    <li><u>fbs</u> - cukier we krwi na czczo;</li>
                    <li><u>exang</u> - dławica piersiowa wywołana wysiłkiem fizycznym.</li>
                </ul>

                <b>Kategorialne uporządkowane</b>:
                <ul>
                    <li><u>restecq</u> - spoczynkowe wyniki elektrokardiograficzne 
                        (0 - norma, 1 - anomalia, 2 - hipertrofia);</li>
                    <li><u>slope</u> - nachylenie szczytowego odcinka ST podczas wysiłku 
                        (0 - wznoszące się, 2 - płaskie, 3 - opadający).</li>
                </ul>
    
                <b>Stricte kategorialne</b>:
                <ul>
                    <li><u>cp</u> - rodzaj bólu w klatce piersiowej (cztery typy);</li>
                    <li><u>thal</u> - thal = niedokrwistość tarczowatokrwinkowa (trzy typy).</li>
                </ul>
            </p>
            <p>Oczywiscie, niektóre atrybute są bardziej istotne, inne mniej. Teraz
                nie możemy stwierdzać nic poza określeniem współczynnika korelacji.
                Jeśli połączymy razem wartości opisujące wiek oraz puls pacjenta, uzyskamy nietrywialny wykres,
                na którym dobrze widać medianę próbek względem stosunku tych liczb:
            </p>
            <div class="plot"><img src="img/som_hd/correlation_1.png"></img></div>
            <h2>Przygotowanie danych</h2>
            <p>W tym rozdziale będziemy pokazywali kluczowe kawałki kodu. Całość można znaleźć tu:
                <a href="#">notatnik Jupyter (Google Colab)</a>
            </p>
            <p>Przede wszystkim powinniśmy jakoś postąpić z brakującymi wartościami. Ze zwględu na to,
                że każdy rekord zawiera 13 cech, to możemy te dwie znane nam kolumny douzupełnić wartością
                średnią. Nie stać nas na to, żeby wyrzucić te rekordy, bo i tak mamy zaledwie
                303 instancje, ani nie stać nas na wyrzucenie całych tych kolumn, gdyż już na etapie
                sprawdzenie zależności liniowych (korelacji) z atrybutem przewidywanym wykazały się
                dosyć poważnie. Uwaga: dozupełniać chcemy teraz, zanim zaczniemy dzielić zbiór na zbiory testowy,
                treningowy i walidacyjny. W żadnym przypadku nie patrzymy na parametry statystyczne, tylko
                mechanicznie rozrzucamy te liczby, by nie podglądać danych, które później zostaną częścią
                zbioru testowego:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Rzutowanie wszystkich kolumn na liczby zmiennoprzecinkowe
                    X = X.astype("float")

                    # Imputacja wartości
                    from sklearn.impute import SimpleImputer
                    import numpy as np
                    
                    imputer = SimpleImputer(missing_values = np.NaN, strategy="median")
                    imputer.fit(X[['ca', 'thal']])
                    imputed_X_columns = imputer.transform(X[['ca', 'thal']])
                    
                    X.loc[:, ('ca', 'thal')] = imputed_X_columns
                </code></pre>
            </div>
            <p>Kolejna rzecz, którą musimy załatwić, polega na przerzucenia wartości przewidywanego
                atrybutu na liczby 0 i 1, gdyż wszystko powyżej 0 wskazuje na obecność choroby i nie 
                dodaje żadnego innego znaczenia. Zróbmy to teraz:
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Zamiana przewidywanego atrybutu na wartości binarne zgodnie z opisem zbioru
                    y.loc[y['num'] > 0, 'num'] = 1.0
                </code></pre>
            </div>
            </p>Następnie na naszej liście zadań do zrobienia mamy oczwywiście przetworzenie atrybutów
            kategorialnych. Tak jak wspomnieliśmy wcześniej, niektóre z nich są binarne, więc z nimi
            nie będzie żadnego problemu, gdyż mieszczą się w przedziale od 0 do 1. Inne mają porządek
            semantyczny i określają stopień konkretnej cechy, więc też je możemy bezpiecznie zostawić, a
            później standaryzować jako atrybuty numeryczne. Zostaje nam problem tych atrybutów, które
            są stricte kategorialne, tzn. nie mają żadnego porządku semantycznego i pokazują typy/kategorii
            konkretnych cech.
            <p>Możemy te atrybutu (<code>cp</code> oraz <code>thal</code>) rozbić na kilka kolumn.
            Każda taka kolumna będzie binarna i będzie określała występowanie poszczególnej kategorii.
            Mówiąc inaczej, kodujemy ten wcześniejszy atrybut kategorialny metodą gorącej jedynki
            (z ang. <b>One-hot Encoding</b>). Poniżej przedstawiony jest kawałek kodu, który to robi
            dla atrybutu <code>cp</code>:
            </p> 
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    X_cp = X[['cp']]
                    
                    # Zakodujmy najpierw "gorącojedynkowo" wszystkie cztery typy 'cp'
                    from sklearn.preprocessing import OneHotEncoder
                    
                    cat_encoder = OneHotEncoder()
                    X_cp_1hot = cat_encoder.fit_transform(X_cp)

                    # Teraz mamy dodatkowe 4 kolumny (każda odpowiada za obecność 
                    # określonego rodzaju bólu w klatce piersiowej):
                    print(X_cp_1hot.toarray())
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    [[1. 0. 0. 0.]
                    [0. 0. 0. 1.]
                    [0. 0. 0. 1.]
                    ...
                    [0. 0. 0. 1.]
                    [0. 1. 0. 0.]
                    [0. 0. 1. 0.]]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Pierwotne etykiety (numery typu bólu, 1-4) mamy wciąż pod ręką
                    cat_encoder.categories_
                </code></pre>
                <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                    [array([1., 2., 3., 4.])]
                </pre>
                <pre class="line-numbers"><code class="language-python">
                    # Dodajmy teraz te typy zakodowane binarnie do naszych danych i wyrzućmy kolumnę 'cp'
                    for i in range(4):
                        X[f"cp_type_{i + 1}"] = X_cp_1hot.toarray()[:, i].reshape(-1, 1)
                    X.drop(columns=["cp"], inplace=True)
                </code></pre>
            </div>
            <p>Podzielmy teraz nasze dane na trzy zbiory: <b>10% testowy, 90% treningowy (w 
                tym 20% walidacyjny)</b>. Dodatkowo,
                zastosujmy standaryzację danych (oprócz atrybutów binarnych):
            <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                from sklearn.model_selection import train_test_split
        
                # Podział na zbióry: testowy, treningowy
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)
                
                # Standaryzacja
                from sklearn.preprocessing import StandardScaler

                num_columns = ['age', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca']
                cat_columns = ['sex', 'fbs', 'exang', 'cp_type_1', 'cp_type_2', 'cp_type_3', 'cp_type_4', 'thal_3', 'thal_6', 'thal_7']
                X_train_cat = X_train.loc[:, cat_columns]
                X_train_num = X_train.loc[:, num_columns]

                std_scaler = StandardScaler()
                X_train_num_scaled = std_scaler.fit_transform(X_train_num)
                X_train = np.concatenate((X_train_num_scaled, X_train_cat.to_numpy()), axis=1)
                
                y_train = y_train.to_numpy()
            </code></pre></div>
            <p>Jeśli chodzi o dane do walidacji, to sensownym rozwiązaniem wydaje się być
                stosowanie K-składową walidacji krzyżowej (z ang. <b>K-Fold Cross Validation</b>):
                <div class="plot"><img src="img/som_hd/k_fold_validation.png"></img></div>
                <div class="code-snippet"><pre class="line-numbers"><code class="language-python">
                    k_folds = 5
                    num_val_samples = len(X_train) // k_folds
                    num_last_val_samples = len(X_train) % k_folds
                    
                    X_validation_sets = []
                    y_validation_sets = []
                    
                    X_train_sets = []
                    y_train_sets = []
                    
                    for i in range(k_folds):
                        print('Processing fold #', i)
                        X_validation_sets.append(X_train[i * num_val_samples: (i + 1) * num_val_samples])
                        y_validation_sets.append(y_train[i * num_val_samples: (i + 1) * num_val_samples])
                        X_train_sets.append(np.concatenate([X_train[:i * num_val_samples],
                        X_train[(i + 1) * num_val_samples:]], 
                        axis=0))
                        y_train_sets.append(np.concatenate([y_train[:i * num_val_samples],
                        y_train[(i + 1) * num_val_samples:]],
                        axis=0))
                    
                        # Dodajemy resztę do ostaniego zbioru walidacyjnego
                        if i == (k_folds - 1):
                        X_validation_sets[k_folds - 1] = np.concatenate((X_validation_sets[k_folds - 1], X_train[- num_last_val_samples:]),
                        axis=0)
                        y_validation_sets[k_folds - 1] = np.concatenate((y_validation_sets[k_folds - 1], y_train[- num_last_val_samples:]),
                        axis=0)
                </code></pre></div>
            <p>Teraz bodajże wszystkie atrybuty mają wartości przedstawione numerycznie, 
                a te, które określają kategorie, mają albo porządek, albo są zakodowane binarnie. 
                Zatem po standaryzacji danych, będziemy mieli zachowane zależności i odległości
                tych danych. Czas na to, żeby uznać zbiór za przygotowany do naszych eksperymentów.
            </p>
            <h2>Struktura sieci SOM</h2>
            <p>Wiemy, oczywiście, że mamy do czynienia z problemem klasyfikacji, więc moglibyśmy 
                od razu w przestrzeni neuronów ustawić tylko dwie jednostki (jedna odpowiadałaby
                wówczas za osób chorych, inny za zdrowych). Jednak nie będzie to
                prawdziwa SOM, tylko zwyczajne wykonanie algorytmu LVQ, stosując uczenie nadzorowane.
        
            <p>Spróbujmy zatem postąpić inaczej. Wyobraźmy sobie, że nie wiemy, że mamy dane naszych 
                pacjęntów i nie wiemy, czy wgl oni się wyodrębniają w jakieś osobne kategorie czy nie. 
                Wtedy mamy uczenie nienadzorowane, więc klastry "osoby chore" i "osoby zdrowe" 
                powstaną naturalnie (co najmniej w teorii, bo możliwe, że sięc nam zamiast tego np.
                rozdzieli ludzi na kobiet i mężczyzn, ale to jest mniej prawdopodobne, bo 17 kolumn 
                wskazują prawdopodobnie na to, że płeć nie stanowi za dużą różnicę w przypadku danych
                medycznych dotyczących chorób serca).

            <p><b>Architektura sieci:</b><p>
            <ul>
                <li><u>Siatka</u> - 2D, 9x9;</li>
                <li><u>Liczba neuronów</u> - 81 (zasada heurystyczna dla treningowego zbioru danych
                    o rozmiarze 272 rekordy);</li>
                <li><u>Wymiar wektorów wag</u> - 18 (co odpowiada liczbie atrybutów);</li>
                <li><u>Odległość</u> - metryka Euklidesowa</li>
                <li><u>Liczenie sąsiedztwa</u> - metryka Manhattan;</li>
                <li><u>Redukcja współczynników</u> - zależy od maksymalnych wartości oraz całkowitej
                liczby iteracji.</li>
            </ul>
            <div class="plot"><img src="img/som_hd/som_nn.png"></img></div>
            <p>Skoro mamy w zbiorze treningowym 272 atrybuty, to zgodnie z zasadą kciuka, która mówi, że
                liczba neuronów to 5 * sqrt(N), lepiej jest tworzyć siatkę 9x9, co właśnie zrobiliśmy. 
            </p>
            <div class="code-snippet">
                <pre class="line-numbers"><code class="language-python">
                    # Inicjalizacja parametrów siatki
                    n_dimensions = 18
                    grid_size = 9
                    
                    # Inicjalizacja wag neuronow (do klasteryzacji)
                    initial_weights = np.zeros((grid_size, grid_size, n_dimensions))
                    for i in range(grid_size):
                    for j in range(grid_size):
                    for k in range(n_dimensions):
                    initial_weights[i, j, k] = np.random.uniform(-1, 1, 1)
                </code></pre>
            </div>
            <p>Metryka Manhattan pozwala na szybkie i proste liczenie promienia obszaru
                sąsiedztwa, gdyż jest to zwykła norma L1 (Lasso).
            </p>
            <h2>Uczenie sieci</h2>
                <p>Algorytmów do wyboru w przypadku sieci SOM nie mamy za dużo. Z definicji sieci
                    jest to tzw. Online learning. Czyli dokładamy siatki neuronów o losowo zainincjalizowanych
                    wagach kolejne próbki i wyłuskujemy neurona-zwycięzcę (mierzymy to za pomocą metryki
                    Euklidesowe: (przypisane wagi neuronu, atrybuty/kolumny próbki)). Może jedyna rzecz,
                    na którą warto zwrócić uwagę to to, że obszar sąsiedztwa operuje nie w przestrzenie wag
                    neuronów (de facto przestrzeni danych wejściowych), tylko na samej siatce, która jest 
                    statyczna, czyli położenie neuronów się nigdy nie zmienia. Na wielu wizualizacjach dostępnych
                    w Internecie często pokazuje się, jak siatka ta rozciąga się i przyjmuje kształt danych wejściowych.
                    Oczywiście w taki sposób należy rozumieć zmianę/dostrajanie wag. Natomiast tak wizualnie, to jest
                    raczej na odwrót. To próbki są rozpinane na siatce rzucane w taki sposób, że każdy rekord z X znajdzie
                    swój neuron w Y. Później patrzymy na siatkę i widzimy (co najmniej mamy taką nadzieję) klastry.
                </p>
                <p><b>Wybrane parametry:</b>
                <p>
                <ul>
                    <li><u>Learning rate</u> - 0.9;</li>
                    <li><u>Maksymalny promień obszaru sąsiedztwa</u> - 10;</li>
                    <li><u>Liczba iteracji ("epoki" * liczba_próbek)</u> - 6528;</li>
                </ul>
                <div class="code-snippet">
                    <pre class="line-numbers"><code class="language-python">
                        def train_SOM(n_epochs, X_part, y_part, weights, n_dimensions, grid_size, show_plots=False,
                                    progress_bar=False, max_lr=.9, max_ds=10):
                            # Hyperparametry modelu
                            max_learning_rate = max_lr
                            learning_rate = max_learning_rate

                            max_distance = max_ds
                            dist = max_distance

                            epochs = n_epochs
                            n_samples = len(X_part)

                            total_n_steps = epochs * n_samples
                            step = 0

                            if show_plots:
                                plot_U_matrix(step, weights, grid_size)
                                plot_labels(step, grid_size, weights, X_part, y_part)
                                plot_labels(step, grid_size, weights, X_part, y_part, majority_voting=True)

                            # Trenowanie sieci SOM
                            for _ in range(epochs):
                                # Iterowanie po wszystkich próbkach z przestrzeni wejściowej
                                for sample_index in range(n_samples):

                                # Aktualizacja współczynnika uczenia się oraz dystansu funkcji sąsiedzstwa
                                learning_rate, dist = decay(step, total_n_steps, max_learning_rate, max_distance)
                                sample = X_part[sample_index]

                                # Liczenie odległości od wszystkich neuronów
                                distances = [[euclidian_distance(sample, weights[i, j]) for j in range(grid_size)] for i in range(grid_size)]

                                # Znalezienie neurona-zwycięzcy
                                winner_index = np.argmin(distances)

                                # Wyłuskanie indeksu zwycięzcy
                                row = winner_index // grid_size
                                col = winner_index % grid_size

                                # Aktualizacja wag wszystkich neuronów
                                for j in range(grid_size):
                                for i in range(grid_size):
                                if man([row, col], [i, j]) <= dist: weights[i, j]=weights[i, j] + learning_rate * (sample - weights[i, j]) 
                                
                                # Rysowanie wykresów 
                                if step % 1000==0: 
                                    if progress_bar: 
                                        sys.stdout.write(f"\rStep {step + 1}\n") 
                                    if show_plots: 
                                        if step % 1000 == 0: 
                                            plot_U_matrix(step + 1, weights, grid_size) 
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part)
                                            plot_labels(step + 1, grid_size, weights, X_part, y_part, majority_voting=True) 
                               
                                # Zliczanie liczby iteracji
                                step=step + 1 
                        
                        # Zwrócenie macierzy, którą będziemy mogli wykorzystać do predykcji return np.copy(weights),
                        get_prediction_matrix(grid_size, weights, X_part, y_part, majority_voting=True)
                    </code></pre>
                </div>
                    <p>Oczywiście dostrajaliśmy parametry, o czym powiemy później. W trakcie trenowani odwołaliśmy się
                        do własnych zdefiniowanych funkcji, które rysowały nam odpowiednie wykresy. Najpierw
                        po każdej iteracji kolorowaliśmy stany neuronów (czerowny - trafił chory pacjent, niebieski - 
                        trafił zdrowy pacjent, szary - jeszcze nikt nie trafił i wagi neurona nie są zmieniane):
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/1.png"></img></div>
                        <div><img src="img/som_hd/2.png"></img></div>
                    </div>
                    <p>Możemy też zobaczyć jak wygląda macierz odległości (U-Matrix). Mapa odległości (U-Matrix) 
                        pokazuje nam odległość pomiędzy wagami sąsiednich neuronów, a to właśnie te wagi dostrajane
                    są do kształtu danych z przestrzeni wejściowej. Zatem im dalej dwa neurony od siebie się znajdują w sensie "wagowym",
                    tym bardziej różniący się od siebie próbki te neurony chwyciły. Na mapie podczas klasteryzacji tworzą się ściany, które
                    separują nam zgrupowane rekordy.</p> 
                    <div class="row-with-images">
                        <div><img src="img/som_hd/3.png"></img></div>
                        <div><img src="img/som_hd/4.png"></img></div>
                    </div>
                    <p>Żeby móc klasyfikować próbki ze zbioru walidacyjnego, użyliśmy tak zwanego Majority Voting, czyli głosowania.
                        Innymi słowy, jeśli do neuronu trafiają 4 czerwone punkty i 8 niebieskich, to uznajemy, że ten neuron nadaje
                        nowym próbkom etykietę "osoba zdrowa". Jeżeli mamy w głosowaniu niejednoznaczność albo neuron w ogóle
                        nie złapał żadnego punktu podczas trenowania, to oznaczamy na szaro taki neuron jako niezainicjalizowany.
                        Podczas klasyfikacji zachowujemy wtedy stabilność statystyczną i próbujemy zgadnąć (czyli prawdopodobieństwo = 50%).
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/5.png"></img></div>
                        <div><img src="img/som_hd/6.png"></img></div>
                    </div>
                    <h2>Wyniki</h2>
                    <p>Możemy zobaczyć końcowy wynik (po wcześniejszej walidacji, oczywiście, którą przedstawimy w kolejnej części):</p>
                    <div class="code-snippet">
                        <pre class="line-numbers"><code class="language-python">
                            from sklearn.metrics import accuracy_score

                            # Końcowa ewaluacja na zbiorze testowym
                            ground_truth_values = y_test
                            predictions = []
                            
                            for sample_index in range(len(X_test)):
                                predictions.append(predict_sample(X_test[sample_index], prediction_matrix, weights))
                                accuracy = accuracy_score(ground_truth_values, predictions)
                                print(f"\tDokładność (zbiór testowy): {accuracy}\n")
                        </code></pre>
                        <pre class="command-line" data-filter-output="(out)" data-continuation-str="\">
                            Dokładność (zbiór testowy): 0.8064516129032258
                        </pre>
                    </div>
                    <h2>Dostrajanie parametrów</h2>
                    <p>Stosujemy algorytm zmniejszenia współczynnika uczenia się oraz dystansu. Prędkość zależy
                        od liczby iteracji. Zatem zakodowane te dwa hyperparametry w tę liczbę, która zmienia nam
                        wyniki walidacji i pozwala wykryć, kiedy należy się zatrzymać, by uniknąć nadmiernego dopasowania.
                    </p> 
                    <div class="plot plot-small-width"><img src="img/som_hd/7.png"></img></div>
                    <p>Możemy też specjalnie użyć za dużej siatki. Wtedy zobaczymy dużo neuronów, które wciąż mają
                        losowe wagi, a więc uznawane są za niezainicjalizowane. Widać również, że z punktu widzenia
                        kwantyzacji macierz odległości próbuje tworzyć za dużo klastrów.
                    </p>
                    <div class="row-with-images">
                        <div><img src="img/som_hd/8.png"></img></div>
                        <div><img src="img/som_hd/9.png"></img></div>
                        <div><img src="img/som_hd/10.png"></img></div>
                    </div>
                </div>
        </div>
    </div>

    <footer>
        <div class="wrapper">
            <p>&copy; 2023 Piotr Dziedzic, Hubert Musiał, Hubert Pamuła, Tair Yerniyazov</p>
        </div>
    </footer>

    <script src="script/script.js"></script>
    <script src="script/prism.js"></script>
</body>

</html>